{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "fed",
      "language": "python",
      "name": "fed"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "federated_stack_overflow_next_word_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oWebvsxIv3QD",
        "CP51uC_3v3QN",
        "9AFI3SMJv3QQ",
        "KTXvamp2v3QU",
        "8mLzdWOmv3QW",
        "7AoGu_Ocv3Qf",
        "wO2A-QP1v3Qk",
        "7jeI0pS2v3Qx",
        "u3renvYrv3Q3",
        "djRJhqvKv3RA",
        "x9V0ez2Uv3RL",
        "TER9MwfKv3RP",
        "lqMZQp4Zv3RZ"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4VQIB2zv3N-",
        "colab_type": "text"
      },
      "source": [
        "# Word Level Federated Text Generation with Stack Overflow (Work in Progress)\n",
        "- Joel Stremmel, Arjun Singh\n",
        "- 01-20-20\n",
        "\n",
        "**About:**\n",
        "\n",
        "This notebook loads the Stack Overflow data available through `tff.simulation.datasets` and trains an LSTM model with Federared Averaging by following the Federated Learning for Text Generation [example notebook](https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb).\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "This notebook prepares the Stack Overflow dataset for word level language modeling using this [module](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/dataset.py\n",
        ").\n",
        "\n",
        "\n",
        "**Data:** \n",
        "- https://www.kaggle.com/stackoverflow/stackoverflow\n",
        "\n",
        "**License:** \n",
        "- https://creativecommons.org/licenses/by-sa/3.0/\n",
        "\n",
        "**Data and Model References:**\n",
        "- https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data\n",
        "- https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb\n",
        "- https://github.com/tensorflow/federated/\n",
        "- https://www.tensorflow.org/tutorials/text/text_generation\n",
        "- https://ruder.io/deep-learning-nlp-best-practices/\n",
        "\n",
        "**Environment Setup References:**\n",
        "- https://www.tensorflow.org/install/gpu\n",
        "- https://gist.github.com/matheustguimaraes/43e0b65aa534db4df2918f835b9b361d\n",
        "- https://www.tensorflow.org/install/source#tested_build_configurations\n",
        "- https://anbasile.github.io/programming/2017/06/25/jupyter-venv/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1BFt-mVv3OB",
        "colab_type": "text"
      },
      "source": [
        "### Environment Setup\n",
        "Pip install these packages in the order listed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO5-Lv57v3OE",
        "colab_type": "code",
        "outputId": "ad6693f3-148b-4e6c-b884-c4cf7f9d5deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade tensorflow-federated\n",
        "!pip install tensorflow\n",
        "!pip install --upgrade tensorflow-gpu==2.0\n",
        "!pip install --upgrade nltk\n",
        "!pip install matplotlib\n",
        "!pip install nest_asyncio"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.0.2\n",
            "Collecting tensorflow-federated\n",
            "  Downloading tensorflow_federated-0.11.0-py2.py3-none-any.whl (385 kB)\n",
            "\u001b[K     |████████████████████████████████| 385 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting tensorflow-model-optimization~=0.1.3\n",
            "  Downloading tensorflow_model_optimization-0.1.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting tensorflow~=2.0.0\n",
            "  Downloading tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3 MB 25 kB/s \n",
            "\u001b[?25hCollecting tensorflow-addons~=0.6.0\n",
            "  Downloading tensorflow_addons-0.6.0-cp36-cp36m-manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 38.2 MB/s \n",
            "\u001b[?25hCollecting grpcio~=1.24.3\n",
            "  Downloading grpcio-1.24.3-cp36-cp36m-manylinux2010_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 35.3 MB/s \n",
            "\u001b[?25hCollecting portpicker~=1.3.1\n",
            "  Downloading portpicker-1.3.1.tar.gz (18 kB)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-federated) (1.12.0)\n",
            "Collecting dm-tree~=0.1.1\n",
            "  Downloading dm_tree-0.1.1-cp36-cp36m-manylinux1_x86_64.whl (289 kB)\n",
            "\u001b[K     |████████████████████████████████| 289 kB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py~=0.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-federated) (0.9.0)\n",
            "Collecting attrs~=18.2\n",
            "  Downloading attrs-18.2.0-py2.py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-privacy~=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-federated) (0.2.2)\n",
            "Collecting cachetools~=3.1.1\n",
            "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-federated) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-federated) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: retrying~=1.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-federated) (1.3.3)\n",
            "Collecting enum34~=1.1\n",
            "  Downloading enum34-1.1.6-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.0.0->tensorflow-federated) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.0.0->tensorflow-federated) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.0.0->tensorflow-federated) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.0.0->tensorflow-federated) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.0.0->tensorflow-federated) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.0.0->tensorflow-federated) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.0.0->tensorflow-federated) (1.1.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 40.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.0.0->tensorflow-federated) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.0.0->tensorflow-federated) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.0.0->tensorflow-federated) (0.8.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
            "\u001b[K     |████████████████████████████████| 449 kB 72.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow-gpu==2.0.0\n",
            "  Downloading tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8 MB 36 kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: mpmath in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy~=0.2.0->tensorflow-federated) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy~=0.2.0->tensorflow-federated) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow~=2.0.0->tensorflow-federated) (42.0.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (3.1.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.11.0-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (0.2.7)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow~=2.0.0->tensorflow-federated) (0.4.8)\n",
            "Building wheels for collected packages: portpicker\n",
            "  Building wheel for portpicker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for portpicker: filename=portpicker-1.3.1-py3-none-any.whl size=13662 sha256=0f65b6614b5d5a27510860aef3fd550918317563a652c07f53a913e03cbb8fc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/f1/26/e6fccc50aa37b340f1b9cc508827ef70b1a2767885f37539ca\n",
            "Successfully built portpicker\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement portpicker~=1.2.0, but you'll have portpicker 1.3.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: enum34, tensorflow-model-optimization, cachetools, google-auth, grpcio, tensorboard, tensorflow-estimator, tensorflow, tensorflow-gpu, tensorflow-addons, portpicker, dm-tree, attrs, tensorflow-federated\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 4.0.0\n",
            "    Uninstalling cachetools-4.0.0:\n",
            "      Successfully uninstalled cachetools-4.0.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.15.0\n",
            "    Uninstalling grpcio-1.15.0:\n",
            "      Successfully uninstalled grpcio-1.15.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "  Attempting uninstall: portpicker\n",
            "    Found existing installation: portpicker 1.2.0\n",
            "    Uninstalling portpicker-1.2.0:\n",
            "      Successfully uninstalled portpicker-1.2.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 19.3.0\n",
            "    Uninstalling attrs-19.3.0:\n",
            "      Successfully uninstalled attrs-19.3.0\n",
            "Successfully installed attrs-18.2.0 cachetools-3.1.1 dm-tree-0.1.1 enum34-1.1.6 google-auth-1.11.0 grpcio-1.24.3 portpicker-1.3.1 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-addons-0.6.0 tensorflow-estimator-2.0.1 tensorflow-federated-0.11.0 tensorflow-gpu-2.0.0 tensorflow-model-optimization-0.1.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cachetools",
                  "enum",
                  "google",
                  "grpc",
                  "portpicker",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.5)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (42.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.11.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.8)\n",
            "Requirement already up-to-date: tensorflow-gpu==2.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (42.0.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.2.7)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.4.8)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449905 sha256=f7a2ff5ef650f1721108b456790977831aa33c0f9e9e53325018e79b80db0f07\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/c9/b0/ed26a73ef75a53145820825afa8e2d2c9b30fe9f6c10cd3202\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.1.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.17.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (42.0.2)\n",
            "Collecting nest_asyncio\n",
            "  Downloading nest_asyncio-1.2.2-py3-none-any.whl (4.5 kB)\n",
            "Installing collected packages: nest-asyncio\n",
            "Successfully installed nest-asyncio-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piwLYIaRv3OJ",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FGMyvi-v3OK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2Be9xTSv3OP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weqfse9iv3OT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/dataset.py\n",
        "from utils.dataset import construct_word_level_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "musiLippv3OW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import functools\n",
        "import six\n",
        "import time\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXYi7z3AwOsK",
        "colab_type": "text"
      },
      "source": [
        "### To get the local files to be imported into Colab Sessions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6t3ZIEll0pr-",
        "outputId": "48082e09-d2e4-4aee-ea88-c38e7e82e0ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "!ls /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "'My Drive'  'Shared drives'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N8yBscwu5Ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/'\n",
        "os.chdir(BASE_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leJoX_gXLar4",
        "colab_type": "code",
        "outputId": "53201b30-c580-40f6-9b78-b51b67e15c09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/colab_files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3ktxJLYxmTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dataset\n",
        "from dataset import construct_word_level_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPMt3HrVv3Of",
        "colab_type": "text"
      },
      "source": [
        "### Set Compatability Behavior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRVs0I62v3Og",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.enable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcF4rXkkv3Oj",
        "colab_type": "text"
      },
      "source": [
        "### Check Tensorflow Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBqre2X0v3Ol",
        "colab_type": "code",
        "outputId": "087c2af9-027e-498c-879b-b431d2099317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('Built with Cuda: {}'.format(tf.test.is_built_with_cuda()))\n",
        "print('Build with GPU support: {}'.format(tf.test.is_built_with_gpu_support()))\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Built with Cuda: True\n",
            "Build with GPU support: True\n",
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seWiooY3v3Op",
        "colab_type": "text"
      },
      "source": [
        "### Set Tensorflow to Use GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7rwvh45v3Oq",
        "colab_type": "code",
        "outputId": "63200608-70b5-4518-bfd9-2d75a99da19d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "physical_devices = tf.config.experimental.list_physical_devices(device_type=None)\n",
        "tf.config.experimental.set_memory_growth(physical_devices[-1], enable=True)\n",
        "for device in physical_devices:\n",
        "    print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
            "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n",
            "PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\n",
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muSX0Xr3v3Ow",
        "colab_type": "text"
      },
      "source": [
        "### Test TFF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH0nlkMIv3Ox",
        "colab_type": "code",
        "outputId": "660e9d2b-9402-448b-cb67-fde505469e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tff.federated_computation(lambda: 'Hello, World!')()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, World!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N26tNyeyEUP",
        "colab_type": "text"
      },
      "source": [
        "### What follows next is the pre-processing for NWP, as now the vocab is in terms of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IScW0y2u0P9f",
        "colab_type": "code",
        "outputId": "3f299d30-03f6-41d7-a0ff-8fbde1bef403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "  # path = tf.keras.utils.get_file(\n",
        "  #     'stackoverflow.tar.bz2',\n",
        "  #     origin='https://storage.googleapis.com/tff-datasets-public/stackoverflow.tar.bz2',\n",
        "  #     file_hash='99eca2f8b8327a09e5fc123979df2d237acbc5e52322f6d86bf523ee47b961a2',\n",
        "  #     hash_algorithm='sha256',\n",
        "  #     extract=True,\n",
        "  #     archive_format='tar',\n",
        "  #     cache_dir=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tff-datasets-public/stackoverflow.tar.bz2\n",
            "9076670464/9076663578 [==============================] - 196s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WAbBZigIHjQ",
        "colab_type": "code",
        "outputId": "76cc815e-c59c-4ea6-9cea-ccf720484280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "  path = tf.keras.utils.get_file(\n",
        "      'shakespeare.tar.bz2',\n",
        "      origin='https://storage.googleapis.com/tff-datasets-public/shakespeare.tar.bz2',\n",
        "      file_hash='0285be9906cb5f268092eee4edeeacfc2af4574f2941f7cc2f08a321d7f5c707',\n",
        "      hash_algorithm='sha256',\n",
        "      extract=True,\n",
        "      archive_format='tar',\n",
        "      cache_dir=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tff-datasets-public/shakespeare.tar.bz2\n",
            "1851392/1848122 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxF4GRkP5j_G",
        "colab_type": "code",
        "outputId": "07ab1f1a-7429-442f-a797-fd8faa389a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets/shakespeare.tar.bz2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMRJdgqc5rx3",
        "colab_type": "code",
        "outputId": "385d95e1-c0cd-453d-e4c5-14fe6e15ac72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install Unidecode"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.1.1-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 36.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 1.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 2.7 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 1.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 2.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71 kB 3.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 238 kB 2.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HouFU9L7yD17",
        "colab_type": "code",
        "outputId": "fe0548e1-fcd4-4073-d6b3-d723f5d7ca12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "import os\n",
        "import time\n",
        " \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import unidecode\n",
        "from keras_preprocessing.text import Tokenizer\n",
        " \n",
        "#tf.enable_eager_execution()\n",
        " \n",
        "#file_path = path\n",
        "\n",
        "text = unidecode.unidecode(open('raw_shakespeare_data.txt').read().encode('utf-8').strip())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-807597dc6438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#file_path = path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'raw_shakespeare_data.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/unidecode/__init__.py\u001b[0m in \u001b[0;36munidecode_expect_ascii\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0m_warn_if_not_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mbytestring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ASCII'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeEncodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'encode'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI_QiAM_RHJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('raw_shakespeare_data.txt') as f:\n",
        "    # This reads all the data from the file, but does not do any processing on it.\n",
        "    data = f.read()\n",
        "\n",
        "# preprocessing to replace all the whitespace characters (space, \\n, \\t, etc.) in the file with the space character.\n",
        "data = \" \".join(data.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0Y6B0sZSagU",
        "colab_type": "code",
        "outputId": "5734a6fc-6718-4c79-d85b-a86c7863d6a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Project Gutenberg EBook of The Complete Works of William Shakespeare, by William Shakespeare Thi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8CwPF_QSYHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        " \n",
        "encoded = tokenizer.texts_to_sequences([text])[0]\n",
        " \n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        " \n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = tokenizer.index_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22mFL_rQSnH5",
        "colab_type": "code",
        "outputId": "42515ee9-4dc1-45c8-a0ad-342808307578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "{k: word2idx[k] for k in list(word2idx)[:5]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 2, 'i': 3, 'of': 5, 'the': 1, 'to': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc97la-9Tl9g",
        "colab_type": "code",
        "outputId": "c8530f78-504d-49bf-fbbd-7c2000bcfef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "{k: idx2word[k] for k in list(idx2word)[:5]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'the', 2: 'and', 3: 'i', 4: 'to', 5: 'of'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f86pglVCTq21",
        "colab_type": "code",
        "outputId": "035cd130-0b7c-4a53-950a-f21d891d04ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27433"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWnEMpczv3O0",
        "colab_type": "text"
      },
      "source": [
        "### Set Some Parameters for Preprocessing the Data and Training the Model\n",
        "**Note:** Ask Keith how he's been setting there for internal experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7g4CoAAv3O2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = vocab_size\n",
        "BATCH_SIZE = 16\n",
        "CLIENTS_EPOCHS_PER_ROUND = 1\n",
        "MAX_SEQ_LENGTH = 100\n",
        "MAX_ELEMENTS_PER_USER = 100\n",
        "CENTRALIZED_TRAIN = False\n",
        "SHUFFLE_BUFFER_SIZE = 5000\n",
        "NUM_VALIDATION_EXAMPLES = 200\n",
        "NUM_TEST_EXAMPLES = 200\n",
        "\n",
        "NUM_ROUNDS = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1VBx_V8v3O5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, val_data, test_data = construct_word_level_datasets(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    client_epochs_per_round=CLIENTS_EPOCHS_PER_ROUND,\n",
        "    max_seq_len=MAX_SEQ_LENGTH,\n",
        "    max_elements_per_user=MAX_ELEMENTS_PER_USER,\n",
        "    centralized_train=CENTRALIZED_TRAIN,\n",
        "    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
        "    num_validation_examples=NUM_VALIDATION_EXAMPLES,\n",
        "    num_test_examples=NUM_TEST_EXAMPLES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwOO9_PXv3PK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-kk08R0v3PO",
        "colab_type": "text"
      },
      "source": [
        "### Count Number of Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVsOmwdTv3PS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('{} train clients.'.format(len(train_data.client_ids)))\n",
        "print('{} val clients.'.format(len(val_data.client_ids)))\n",
        "print('{} test clients.'.format(len(test_data.client_ids)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQBHhZgMv3PZ",
        "colab_type": "text"
      },
      "source": [
        "### Set Vocabulary\n",
        "- Currently using the fixed vocabularly of ASCII chars that occur in the works of Shakespeare and Dickens\n",
        "- **Is there a good way to get the distinct characters from a TF dataset?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEKER-0cv3Pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = list('dhlptx@DHLPTX $(,048cgkoswCGKOSW[_#\\'/37;?bfjnrvzBFJNRVZ\"&*.26:\\naeimquyAEIMQUY]!%)-159\\r')\n",
        "vocab_size = len(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AJNGqMDv3Pi",
        "colab_type": "text"
      },
      "source": [
        "### Creating a Mapping from Unique Characters to Indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnq11e9Lv3Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSdZTrkqv3Pl",
        "colab_type": "text"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du2cCJydv3Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(batch_size, vocab_size, seq_length, embedding_dim=256, rnn_units=512):\n",
        "    \"\"\"\n",
        "    Build model with architecture from: https://www.tensorflow.org/tutorials/text/text_generation.\n",
        "    \"\"\"\n",
        "\n",
        "    model1_input = tf.keras.Input(shape=(seq_length, ),\n",
        "                                  name='model1_input')\n",
        "    \n",
        "    model1_embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                 output_dim=embedding_dim,\n",
        "                                                 input_length=seq_length,\n",
        "                                                 batch_input_shape=[batch_size, None],\n",
        "                                                 name='model1_embedding')(model1_input)\n",
        "    \n",
        "    model1_lstm = tf.keras.layers.LSTM(units=rnn_units,\n",
        "                                       return_sequences=True,\n",
        "                                       recurrent_initializer='glorot_uniform',\n",
        "                                       name='model1_lstm')(model1_embedding)\n",
        "    \n",
        "    model1_dense = tf.keras.layers.Dense(units=vocab_size)(model1_lstm)\n",
        "    \n",
        "    final_model = tf.keras.Model(inputs=model1_input, outputs=model1_dense)\n",
        "                 \n",
        "    return final_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wmWjqfbv3Po",
        "colab_type": "text"
      },
      "source": [
        "### Define the Text Generation Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYExalZ6v3Pq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    \"\"\"\n",
        "    Generate text by sampling from the model output distribution\n",
        "    as in From https://www.tensorflow.org/tutorials/sequences/text_generation.\n",
        "    \"\"\"\n",
        "\n",
        "    num_generate = 200\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "    temperature = 1.0\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-UiHplXv3Pt",
        "colab_type": "text"
      },
      "source": [
        "### Load or Build the Model\n",
        "Text generation requires a batch_size=1 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mHixnRkNv3Pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_model_batch1 = build_model(batch_size=1, vocab_size=vocab_size, seq_length=SEQ_LENGTH)\n",
        "print(generate_text(keras_model_batch1, \"How's the water today? \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuzh7xqCv3Px",
        "colab_type": "text"
      },
      "source": [
        "### Define Functions to Preprocess Federated Stack Overflow\n",
        "- Using a namedtuple with keys x and y as the output type of the dataset keeps both TFF and Keras happy.\n",
        "- Construct a lookup table to map string chars to indexes, using the vocab loaded above.\n",
        "- Write functions for:\n",
        "    - ID lookup\n",
        "    - Splitting inputs and targets\n",
        "    - Applying preprocessing steps to dataset\n",
        "    - Taking clients and client records and applying preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8kS2l9Hv3Py",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BatchType = collections.namedtuple('BatchType', ['x', 'y'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0OJqNXWv3P0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = tf.lookup.StaticHashTable(\n",
        "    tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=vocab,\n",
        "        values=tf.constant(list(range(len(vocab))),\n",
        "        dtype=tf.int64)),\n",
        "    default_value=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUl6LPmWv3P3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_ids(x):\n",
        "    \n",
        "    s = tf.reshape(x['tokens'], shape=[1])\n",
        "    chars = tf.strings.bytes_split(s).values\n",
        "    ids = table.lookup(chars)\n",
        "    \n",
        "    return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyIDUxXnv3P7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    \n",
        "    input_text = tf.map_fn(lambda x: x[:-1], chunk)\n",
        "    target_text = tf.map_fn(lambda x: x[1:], chunk)\n",
        "    \n",
        "    return BatchType(input_text, target_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOnQ-LWKv3P9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(dataset):\n",
        "    \n",
        "    return (\n",
        "        # Map ASCII chars to int64 indexes using the vocab\n",
        "        dataset.map(to_ids)\n",
        "        # Split into individual chars\n",
        "        .unbatch()\n",
        "        # Form example sequences of SEQ_LENGTH +1\n",
        "        .batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
        "        # Shuffle and form minibatches\n",
        "        .shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "        # And finally split into (input, target) tuples,\n",
        "        # each of length SEQ_LENGTH.\n",
        "        .map(split_input_target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_9PgFKXv3QA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_data_for_client(client, data_source):\n",
        "    \n",
        "    return preprocess(data_source.create_tf_dataset_for_client(client))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWebvsxIv3QD",
        "colab_type": "text"
      },
      "source": [
        "### Get Sample Clients for Validation and Testing\n",
        "Note that the train clients will be sampled within the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbrzb7ABv3QF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sample_clients(dataset, num_clients):\n",
        "    \n",
        "    random_indices = np.random.choice(len(dataset.client_ids), size=num_clients, replace=False)\n",
        "    \n",
        "    return np.array(dataset.client_ids)[random_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sImj-72Gv3QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_clients = get_sample_clients(val_data, num_clients=NUM_VAL_CLIENTS)\n",
        "test_clients = get_sample_clients(test_data, num_clients=NUM_TEST_CLIENTS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP51uC_3v3QN",
        "colab_type": "text"
      },
      "source": [
        "### Build and Preprocess the Validation and Test Datasets\n",
        "Concatenate the validation and test datasets for evaluation with Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9YQee0mv3QO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset = functools.reduce(lambda d1, d2: d1.concatenate(d2), \n",
        "                               [preprocess_data_for_client(client, val_data) for client in val_clients])\n",
        "\n",
        "test_dataset = functools.reduce(lambda d1, d2: d1.concatenate(d2), \n",
        "                                [preprocess_data_for_client(client, test_data) for client in test_clients])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AFI3SMJv3QQ",
        "colab_type": "text"
      },
      "source": [
        "### Define Lists to Track Loss and Accuracy at Each Training Round"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lXqkQC6v3QR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = []\n",
        "train_accuracy = []\n",
        "val_loss = []\n",
        "val_accuracy = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTXvamp2v3QU",
        "colab_type": "text"
      },
      "source": [
        "### Define the Evaluation Function to Use During Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUR1sqjGv3QU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def keras_evaluate(keras_model, state, val_dataset):\n",
        "    \n",
        "    tff.learning.assign_weights_to_keras_model(keras_model, state.model)\n",
        "    loss, accuracy = keras_model.evaluate(val_dataset, steps=2)\n",
        "    \n",
        "    val_loss.append(loss)\n",
        "    val_accuracy.append(accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mLzdWOmv3QW",
        "colab_type": "text"
      },
      "source": [
        "### Define Loss Function and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4YcR3fhv3QX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FlattenedCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n",
        "\n",
        "    def __init__(self, name='accuracy', dtype=None):\n",
        "        super(FlattenedCategoricalAccuracy, self).__init__(name, dtype=dtype)\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        \n",
        "        y_true = tf.reshape(y_true, [-1, 1])\n",
        "        y_pred = tf.reshape(y_pred, [-1, len(vocab), 1])\n",
        "        \n",
        "        return super(FlattenedCategoricalAccuracy, self).update_state(y_true, y_pred, sample_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqMrz4J2v3Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compile(keras_model):\n",
        "    \n",
        "    keras_model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[FlattenedCategoricalAccuracy()]\n",
        "    )\n",
        "    \n",
        "    return keras_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AoGu_Ocv3Qf",
        "colab_type": "text"
      },
      "source": [
        "### Load and Compile the Model\n",
        "The keras model will be accessed as a global variable to create a copy to be called by TFF and will be updated within the training loop to follow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2TkifSiv3Qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_model = build_model(batch_size=BATCH_SIZE,\n",
        "                          vocab_size=vocab_size,\n",
        "                          seq_length=SEQ_LENGTH)\n",
        "compile(keras_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU6nFobWv3Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO2A-QP1v3Qk",
        "colab_type": "text"
      },
      "source": [
        "### Create TFF Version of the Model to be Trained with Federated Averaging\n",
        "- Clone the keras_model inside `create_tff_model()`, which TFF will call to produce a new copy of the model inside the graph that it will serialize.\n",
        "- TFF uses a `dummy_batch` so it knows the types and shapes that your model expects.\n",
        "- Build and serialize the Tensorflow graph with `build_federated_averaging_process`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPTkBX0Ev3Qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tff_model():\n",
        "    \n",
        "    x = tf.constant(np.random.randint(1, len(vocab), size=[BATCH_SIZE, SEQ_LENGTH]))\n",
        "    dummy_batch = collections.OrderedDict([('x', x), ('y', x)]) \n",
        "    keras_model_clone = compile(tf.keras.models.clone_model(keras_model))\n",
        "    \n",
        "    return tff.learning.from_compiled_keras_model(keras_model_clone, dummy_batch=dummy_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWTjX7_pv3Qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fed_avg = tff.learning.build_federated_averaging_process(model_fn=create_tff_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jeI0pS2v3Qx",
        "colab_type": "text"
      },
      "source": [
        "### Initialize the Federated Averaging Process and the Starting Model State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVC5-Hizv3Qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOTE: If the statement below fails, it means that you are\n",
        "# using an older version of TFF without the high-performance\n",
        "# executor stack. Call `tff.framework.set_default_executor()`\n",
        "# instead to use the default reference runtime.\n",
        "if six.PY3:\n",
        "    tff.framework.set_default_executor(tff.framework.create_local_executor())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAHV8V1cv3Q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The state of the FL server, containing the model and optimization state.\n",
        "state = fed_avg.initialize()\n",
        "\n",
        "state = tff.learning.state_with_new_model_weights(\n",
        "    state,\n",
        "    trainable_weights=[v.numpy() for v in keras_model.trainable_weights],\n",
        "    non_trainable_weights=[v.numpy() for v in keras_model.non_trainable_weights]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3renvYrv3Q3",
        "colab_type": "text"
      },
      "source": [
        "### Train Model Across Many Randomly Sampled Clients with Federated Averaging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "R1vxY6fnv3Q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for round_num in range(NUM_ROUNDS):\n",
        "    \n",
        "    # Examine validation metrics\n",
        "    print(f'Evaluating before training round #{round_num} on {NUM_VAL_CLIENTS} clients.')\n",
        "    keras_evaluate(keras_model, state, val_dataset)\n",
        "    \n",
        "    # Sample train clients to create a train dataset\n",
        "    print(f'Sampling {NUM_TRAIN_CLIENTS} new clients.')\n",
        "    train_clients = get_sample_clients(train_data, num_clients=NUM_TRAIN_CLIENTS)\n",
        "    train_datasets = [preprocess_data_for_client(client, train_data) for client in train_clients]\n",
        "    \n",
        "    # Apply federated training round\n",
        "    print('Applying federated training round.')\n",
        "    state, metrics = fed_avg.next(state, train_datasets)\n",
        "    \n",
        "    # Examine training metrics\n",
        "    print(f'Training metrics - loss: {metrics[1]:4.4f}; accuracy: {metrics[0]:4.4f}')\n",
        "    train_loss.append(metrics[1])\n",
        "    train_accuracy.append(metrics[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djRJhqvKv3RA",
        "colab_type": "text"
      },
      "source": [
        "### Plot Model Objective Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psTPruN-v3RJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "x_axis = range(0, NUM_ROUNDS)\n",
        "ax.plot(x_axis, train_loss, label='Train')\n",
        "ax.plot(x_axis, val_loss, label='Validation')\n",
        "ax.legend(loc='best')\n",
        "plt.ylabel('Value of Objective Function')\n",
        "plt.title('Model Objective Function at Each Training Round')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9V0ez2Uv3RL",
        "colab_type": "text"
      },
      "source": [
        "### Plot Model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOouuxODv3RM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "x_axis = range(0, NUM_ROUNDS)\n",
        "ax.plot(x_axis, train_accuracy, label='Train')\n",
        "ax.plot(x_axis, val_accuracy, label='Validation')\n",
        "ax.legend(loc='best')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy at Each Training Round')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TER9MwfKv3RP",
        "colab_type": "text"
      },
      "source": [
        "### Get Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfXALNjav3RQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_evaluate(keras_model, state, val_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMZQp4Zv3RZ",
        "colab_type": "text"
      },
      "source": [
        "### Generate Text\n",
        "Text generation requires batch_size=1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj8GsmdSv3Ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_model_batch1.set_weights([v.numpy() for v in keras_model.weights])\n",
        "print(generate_text(keras_model_batch1, \"How's the water today? \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7atH3CPv3Re",
        "colab_type": "text"
      },
      "source": [
        "**Suggested extensions:**\n",
        "\n",
        "- Use \".repeat(NUM_EPOCHS)\" on the client datasets to try multiple epochs of local training (e.g., as in McMahan et. al.). See also Federated Learning for Image Classification which does this.\n",
        "- Change the compile() command to experiment with using different optimization algorithms on the client.\n",
        "- Try the server_optimizer argument to build_federated_averaging_process to try different algorithms for applying the model updates on the server.\n",
        "- Try the client_weight_fn argument to to build_federated_averaging_process to try different weightings of the clients. The default weights client updates by the number of examples on the client, but you can do e.g. client_weight_fn=lambda _: tf.constant(1.0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6v0KPthv3Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvKqAv08v3Rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}