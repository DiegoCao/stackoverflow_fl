{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Level Federated Text Generation with Stack Overflow\n",
    "- Joel Stremmel\n",
    "- 01-28-20\n",
    "\n",
    "**About:**\n",
    "\n",
    "This notebook loads the Stack Overflow data available through `tff.simulation.datasets` and trains an LSTM model with Federared Averaging by following the Federated Learning for Text Generation [example notebook](https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb).\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- This notebook prepares the Stack Overflow dataset for word level language modeling using this [module](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/dataset.py\n",
    ").\n",
    "- The metrics for model training come from this [module](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/metrics.py). \n",
    "\n",
    "\n",
    "**Data:** \n",
    "- https://www.kaggle.com/stackoverflow/stackoverflow\n",
    "\n",
    "**License:** \n",
    "- https://creativecommons.org/licenses/by-sa/3.0/\n",
    "\n",
    "**Data and Model References:**\n",
    "- https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data\n",
    "- https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb\n",
    "- https://github.com/tensorflow/federated/\n",
    "- https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/baselines/stackoverflow\n",
    "- https://www.tensorflow.org/tutorials/text/text_generation\n",
    "- https://ruder.io/deep-learning-nlp-best-practices/\n",
    "\n",
    "**Environment Setup References:**\n",
    "- https://www.tensorflow.org/install/gpu\n",
    "- https://gist.github.com/matheustguimaraes/43e0b65aa534db4df2918f835b9b361d\n",
    "- https://www.tensorflow.org/install/source#tested_build_configurations\n",
    "- https://anbasile.github.io/programming/2017/06/25/jupyter-venv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Pip install these packages in the order listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade tensorflow-federated\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip install --upgrade tensorflow-gpu==2.0\n",
    "# !pip install --upgrade nltk\n",
    "# !pip install matplotlib\n",
    "# !pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/dataset.py\n",
    "from utils.dataset import construct_word_level_datasets, get_vocab, get_special_tokens\n",
    "\n",
    "# from https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/metrics.py\n",
    "import utils.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import six\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Compatability Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Tensorflow Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built with Cuda: True\n",
      "Build with GPU support: True\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print('Built with Cuda: {}'.format(tf.test.is_built_with_cuda()))\n",
    "print('Build with GPU support: {}'.format(tf.test.is_built_with_gpu_support()))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Tensorflow to Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices(device_type=None)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[-1], enable=True)\n",
    "for device in physical_devices:\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Some Parameters for Preprocessing the Data and Training the Model\n",
    "**Note:** Ask Keith how he's been setting there for internal experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "BATCH_SIZE = 16\n",
    "CLIENTS_EPOCHS_PER_ROUND = 1\n",
    "MAX_SEQ_LENGTH = 100\n",
    "MAX_ELEMENTS_PER_USER = 100\n",
    "CENTRALIZED_TRAIN = False\n",
    "SHUFFLE_BUFFER_SIZE = 5000\n",
    "NUM_VALIDATION_EXAMPLES = 200\n",
    "NUM_TEST_EXAMPLES = 200\n",
    "\n",
    "NUM_ROUNDS = 10\n",
    "NUM_TRAIN_CLIENTS = 10\n",
    "UNIFORM_WEIGHTING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Word Level Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel_stremmel/anaconda3/envs/tff/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = construct_word_level_datasets(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    client_epochs_per_round=CLIENTS_EPOCHS_PER_ROUND,\n",
    "    max_seq_len=MAX_SEQ_LENGTH,\n",
    "    max_elements_per_user=MAX_ELEMENTS_PER_USER,\n",
    "    centralized_train=CENTRALIZED_TRAIN,\n",
    "    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
    "    num_validation_examples=NUM_VALIDATION_EXAMPLES,\n",
    "    num_test_examples=NUM_TEST_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Dataset Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Special Characters Created During Preprocessing\n",
    "The four special tokens are:\n",
    "- pad: padding token\n",
    "- oov: out of vocabulary\n",
    "- bos: begin of sentence\n",
    "- eos: end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad, oov, bos, eos = get_special_tokens(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "special2idx = dict(zip(['pad', 'oov', 'bos', 'eos'], [pad, oov, bos, eos]))\n",
    "idx2special = {v:k for k, v in special2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Vocabulary\n",
    "Add one to account for the pad token which has idx 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word:i+1 for i, word in enumerate(vocab)}\n",
    "idx2word = {i+1:word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {**word2idx, **special2idx}\n",
    "idx2word = {**idx2word, **idx2special}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Vocab Size\n",
    "This accounts for having added the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = VOCAB_SIZE + len(special2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_dim=256, rnn_units=512):\n",
    "    \"\"\"\n",
    "    Build model with architecture from: https://www.tensorflow.org/tutorials/text/text_generation.\n",
    "    \"\"\"\n",
    "\n",
    "    model1_input = tf.keras.Input(shape=(MAX_SEQ_LENGTH, ),\n",
    "                                  name='model1_input')\n",
    "    \n",
    "    model1_embedding = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                                 output_dim=embedding_dim,\n",
    "                                                 input_length=MAX_SEQ_LENGTH,\n",
    "                                                 batch_input_shape=[BATCH_SIZE, None],\n",
    "                                                 name='model1_embedding')(model1_input)\n",
    "    \n",
    "    model1_lstm = tf.keras.layers.LSTM(units=rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       recurrent_initializer='glorot_uniform',\n",
    "                                       name='model1_lstm')(model1_embedding)\n",
    "    \n",
    "    model1_dense = tf.keras.layers.Dense(units=VOCAB_SIZE)(model1_lstm)\n",
    "    \n",
    "    final_model = tf.keras.Model(inputs=model1_input, outputs=model1_dense)\n",
    "                 \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Text Generation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \"\"\"\n",
    "    Generate text by sampling from the model output distribution\n",
    "    as in From https://www.tensorflow.org/tutorials/sequences/text_generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_words = [word.lower() for word in start_string.split(' ')]\n",
    "\n",
    "    num_generate = 50\n",
    "    input_eval = [word2idx[word] for word in start_words]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2word[predicted_id])\n",
    "\n",
    "    return (' '.join(start_words) + ' '.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you todaylooked repeating interpretation wondering year usual relation cities keys ts concept ^ cleanest limiting ? operations assistance exports stored authorize essential yahoo compile www positions explanation proceed recall migrate msdn broke confuse into java covered datasource primary determines trouble bottleneck relative styled selected concise messaging natural universal price over row\n"
     ]
    }
   ],
   "source": [
    "keras_model_batch1 = build_model()\n",
    "print(generate_text(keras_model_batch1, \"How are you today\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFF Version of the Model to be Trained with Federated Averaging\n",
    "TFF uses a sample batch so it knows the types and shapes that your model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel_stremmel/anaconda3/envs/tff/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    }
   ],
   "source": [
    "sample_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Create TFF model from compiled Keras model and a sample batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    keras_model = build_model()\n",
    "    \n",
    "    train_metrics = [\n",
    "        metrics.NumTokensCounter(name='num_tokens', masked_tokens=[pad]),\n",
    "        metrics.NumTokensCounter(name='num_tokens_no_oov', masked_tokens=[pad, oov]),\n",
    "        metrics.NumBatchesCounter(),\n",
    "        metrics.NumExamplesCounter(),\n",
    "        metrics.MaskedCategoricalAccuracy(name='accuracy', masked_tokens=[pad]),\n",
    "        metrics.MaskedCategoricalAccuracy(name='accuracy_no_oov', masked_tokens=[pad, oov]),\n",
    "        metrics.MaskedCategoricalAccuracy(name='accuracy_no_oov_no_eos', masked_tokens=[pad, oov, eos])\n",
    "    ]\n",
    "    \n",
    "    keras_model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=train_metrics)\n",
    "    \n",
    "    return tff.learning.from_compiled_keras_model(keras_model, sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Lists to Track Loss and Accuracy at Each Training Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "val_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Evaluate Model Performance on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_evaluate(keras_model, state, val_dataset):\n",
    "    \n",
    "    tff.learning.assign_weights_to_keras_model(keras_model, state.model)\n",
    "    loss, accuracy = keras_model.evaluate(val_dataset, steps=2)\n",
    "    \n",
    "    val_loss.append(loss)\n",
    "    val_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Weight Clients Uniformly or by Number of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_weight_fn(local_outputs):\n",
    "    \n",
    "    num_tokens = tf.cast(tf.squeeze(local_outputs['num_tokens']), tf.float32)\n",
    "    \n",
    "    return 1.0 if UNIFORM_WEIGHTING else num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Supply Server Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_optimizer_fn():\n",
    "    \n",
    "    return tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Create Training Datsets from Randomly Sampled Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_clients(dataset, num_clients):\n",
    "    \n",
    "    random_indices = np.random.choice(len(dataset.client_ids), size=num_clients, replace=False)\n",
    "    \n",
    "    return np.array(dataset.client_ids)[random_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model Across Many Randomly Sampled Clients with Federated Averaging\n",
    "- Set the default executor\n",
    "- Create and initailize an iterative process\n",
    "- Apply federated training rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff.framework.set_default_executor(tff.framework.create_local_executor(max_fanout=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/joel_stremmel/anaconda3/envs/tff/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "iterative_process = (\n",
    "      tff.learning.federated_averaging.build_federated_averaging_process(\n",
    "          model_fn=model_fn,\n",
    "          server_optimizer_fn=server_optimizer_fn,\n",
    "          client_weight_fn=client_weight_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 10 new clients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel_stremmel/anaconda3/envs/tff/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1\n",
      "   Loss: 8.15574360\n",
      "   num_batches: 66\n",
      "   num_examples: 945\n",
      "   num_tokens: 14835\n",
      "   num_tokens_no_oov: 14307\n",
      "   accuracy: 0.00054\n",
      "   accuracy_no_oov: 0.00056\n",
      "Sampling 10 new clients.\n",
      "Round: 2\n",
      "   Loss: 5.83773422\n",
      "   num_batches: 68\n",
      "   num_examples: 973\n",
      "   num_tokens: 14345\n",
      "   num_tokens_no_oov: 13656\n",
      "   accuracy: 0.00014\n",
      "   accuracy_no_oov: 0.00015\n",
      "Sampling 10 new clients.\n",
      "Round: 3\n",
      "   Loss: 3.78267336\n",
      "   num_batches: 70\n",
      "   num_examples: 997\n",
      "   num_tokens: 17429\n",
      "   num_tokens_no_oov: 16627\n",
      "   accuracy: 0.00000\n",
      "   accuracy_no_oov: 0.00000\n",
      "Sampling 10 new clients.\n",
      "Round: 4\n",
      "   Loss: 2.38870931\n",
      "   num_batches: 58\n",
      "   num_examples: 825\n",
      "   num_tokens: 12416\n",
      "   num_tokens_no_oov: 11904\n",
      "   accuracy: 0.00000\n",
      "   accuracy_no_oov: 0.00000\n",
      "Sampling 10 new clients.\n",
      "Round: 5\n",
      "   Loss: 1.67560375\n",
      "   num_batches: 68\n",
      "   num_examples: 979\n",
      "   num_tokens: 14771\n",
      "   num_tokens_no_oov: 14073\n",
      "   accuracy: 0.00000\n",
      "   accuracy_no_oov: 0.00000\n",
      "Sampling 10 new clients.\n",
      "Round: 6\n",
      "   Loss: 1.45487618\n",
      "   num_batches: 65\n",
      "   num_examples: 924\n",
      "   num_tokens: 13723\n",
      "   num_tokens_no_oov: 13031\n",
      "   accuracy: 0.00000\n",
      "   accuracy_no_oov: 0.00000\n",
      "Sampling 10 new clients.\n",
      "Round: 7\n",
      "   Loss: 1.33979011\n",
      "   num_batches: 64\n",
      "   num_examples: 914\n",
      "   num_tokens: 13249\n",
      "   num_tokens_no_oov: 12603\n",
      "   accuracy: 0.00000\n",
      "   accuracy_no_oov: 0.00000\n",
      "Sampling 10 new clients.\n",
      "Round: 8\n",
      "   Loss: 1.33621335\n",
      "   num_batches: 61\n",
      "   num_examples: 865\n",
      "   num_tokens: 12940\n",
      "   num_tokens_no_oov: 12257\n",
      "   accuracy: 0.00008\n",
      "   accuracy_no_oov: 0.00000\n",
      "Sampling 10 new clients.\n",
      "Round: 9\n",
      "   Loss: 1.38804615\n",
      "   num_batches: 70\n",
      "   num_examples: 1000\n",
      "   num_tokens: 15236\n",
      "   num_tokens_no_oov: 14463\n",
      "   accuracy: 0.00000\n",
      "   accuracy_no_oov: 0.00000\n"
     ]
    }
   ],
   "source": [
    "for round_num in range(1, NUM_ROUNDS):\n",
    "    \n",
    "    # Examine validation metrics\n",
    "#     print(f'Evaluating before training round #{round_num} on {NUM_VALIDATION_EXAMPLES} clients.')\n",
    "#     keras_evaluate(keras_model, state, val_data)\n",
    "    \n",
    "    # Sample train clients to create a train dataset\n",
    "    print(f'Sampling {NUM_TRAIN_CLIENTS} new clients.')\n",
    "    train_clients = get_sample_clients(train_data, num_clients=NUM_TRAIN_CLIENTS)\n",
    "    train_datasets = [train_data.create_tf_dataset_for_client(client) for client in train_clients]\n",
    "    \n",
    "    # Apply federated training round\n",
    "    server_state, server_metrics = iterative_process.next(server_state, train_datasets)\n",
    "    \n",
    "    # Examine training metrics\n",
    "    print('Round: {}'.format(round_num))\n",
    "    print('   Loss: {:.8f}'.format(server_metrics.loss))\n",
    "    print('   num_batches: {}'.format(server_metrics.num_batches))\n",
    "    print('   num_examples: {}'.format(server_metrics.num_examples))\n",
    "    print('   num_tokens: {}'.format(server_metrics.num_tokens))\n",
    "    print('   num_tokens_no_oov: {}'.format(server_metrics.num_tokens_no_oov))\n",
    "    print('   accuracy: {:.5f}'.format(server_metrics.accuracy))\n",
    "    print('   accuracy_no_oov: {:.5f}'.format(server_metrics.accuracy_no_oov))\n",
    "    \n",
    "    train_loss.append(server_metrics.loss)\n",
    "    train_accuracy.append(server_metrics.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Model Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_axis = range(0, NUM_ROUNDS)\n",
    "ax.plot(x_axis, train_loss, label='Train')\n",
    "ax.plot(x_axis, val_loss, label='Validation')\n",
    "ax.legend(loc='best')\n",
    "plt.ylabel('Value of Objective Function')\n",
    "plt.title('Model Objective Function at Each Training Round')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_axis = range(0, NUM_ROUNDS)\n",
    "ax.plot(x_axis, train_accuracy, label='Train')\n",
    "ax.plot(x_axis, val_accuracy, label='Validation')\n",
    "ax.legend(loc='best')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy at Each Training Round')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_evaluate(keras_model, state, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text\n",
    "Text generation requires batch_size=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_model_batch1.set_weights([v.numpy() for v in keras_model.weights])\n",
    "# print(generate_text(keras_model_batch1, \"How are you today? \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed",
   "language": "python",
   "name": "fed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
