{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Level Federated Text Generation with Stack Overflow\n",
    "- Joel Stremmel\n",
    "- 01-27-20\n",
    "\n",
    "**About:**\n",
    "\n",
    "This notebook loads the Stack Overflow data available through `tff.simulation.datasets` and trains an LSTM model with Federared Averaging by following the Federated Learning for Text Generation [example notebook](https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb).\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "This notebook prepares the Stack Overflow dataset for word level language modeling using this [module](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/dataset.py\n",
    ").\n",
    "\n",
    "\n",
    "**Data:** \n",
    "- https://www.kaggle.com/stackoverflow/stackoverflow\n",
    "\n",
    "**License:** \n",
    "- https://creativecommons.org/licenses/by-sa/3.0/\n",
    "\n",
    "**Data and Model References:**\n",
    "- https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data\n",
    "- https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb\n",
    "- https://github.com/tensorflow/federated/\n",
    "- https://www.tensorflow.org/tutorials/text/text_generation\n",
    "- https://ruder.io/deep-learning-nlp-best-practices/\n",
    "\n",
    "**Environment Setup References:**\n",
    "- https://www.tensorflow.org/install/gpu\n",
    "- https://gist.github.com/matheustguimaraes/43e0b65aa534db4df2918f835b9b361d\n",
    "- https://www.tensorflow.org/install/source#tested_build_configurations\n",
    "- https://anbasile.github.io/programming/2017/06/25/jupyter-venv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Pip install these packages in the order listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade tensorflow-federated\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip install --upgrade tensorflow-gpu==2.0\n",
    "# !pip install --upgrade nltk\n",
    "# !pip install matplotlib\n",
    "# !pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/dataset.py\n",
    "from utils.dataset import construct_word_level_datasets, get_vocab, get_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import six\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Compatability Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Tensorflow Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built with Cuda: True\n",
      "Build with GPU support: True\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print('Built with Cuda: {}'.format(tf.test.is_built_with_cuda()))\n",
    "print('Build with GPU support: {}'.format(tf.test.is_built_with_gpu_support()))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Tensorflow to Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices(device_type=None)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[-1], enable=True)\n",
    "for device in physical_devices:\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Some Parameters for Preprocessing the Data and Training the Model\n",
    "**Note:** Ask Keith how he's been setting there for internal experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "BATCH_SIZE = 16\n",
    "CLIENTS_EPOCHS_PER_ROUND = 1\n",
    "MAX_SEQ_LENGTH = 100\n",
    "MAX_ELEMENTS_PER_USER = 100\n",
    "CENTRALIZED_TRAIN = False\n",
    "SHUFFLE_BUFFER_SIZE = 5000\n",
    "NUM_VALIDATION_EXAMPLES = 200\n",
    "NUM_TEST_EXAMPLES = 200\n",
    "\n",
    "NUM_ROUNDS = 10\n",
    "NUM_TRAIN_CLIENTS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Word Level Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel_stremmel/anaconda3/envs/tff/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = construct_word_level_datasets(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    client_epochs_per_round=CLIENTS_EPOCHS_PER_ROUND,\n",
    "    max_seq_len=MAX_SEQ_LENGTH,\n",
    "    max_elements_per_user=MAX_ELEMENTS_PER_USER,\n",
    "    centralized_train=CENTRALIZED_TRAIN,\n",
    "    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
    "    num_validation_examples=NUM_VALIDATION_EXAMPLES,\n",
    "    num_test_examples=NUM_TEST_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Dataset Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Special Characters Created During Preprocessing\n",
    "The four special tokens are:\n",
    "- pad: padding token\n",
    "- oov: out of vocabulary\n",
    "- bos: begin of sentence\n",
    "- eos: end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "special2idx = dict(zip(['pad', 'oov', 'bos', 'eos'], get_special_tokens(VOCAB_SIZE)))\n",
    "idx2special = {v:k for k, v in special2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Vocabulary\n",
    "Add one to account for the pad token which has idx 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word:i+1 for i, word in enumerate(vocab)}\n",
    "idx2word = {i+1:word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {**word2idx, **special2idx}\n",
    "idx2word = {**idx2word, **idx2special}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Vocab Size\n",
    "This accounts for having added the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = VOCAB_SIZE + len(special2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(batch_size, vocab_size, seq_length, embedding_dim=256, rnn_units=512):\n",
    "    \"\"\"\n",
    "    Build model with architecture from: https://www.tensorflow.org/tutorials/text/text_generation.\n",
    "    \"\"\"\n",
    "\n",
    "    model1_input = tf.keras.Input(shape=(seq_length, ),\n",
    "                                  name='model1_input')\n",
    "    \n",
    "    model1_embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                 output_dim=embedding_dim,\n",
    "                                                 input_length=seq_length,\n",
    "                                                 batch_input_shape=[batch_size, None],\n",
    "                                                 name='model1_embedding')(model1_input)\n",
    "    \n",
    "    model1_lstm = tf.keras.layers.LSTM(units=rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       recurrent_initializer='glorot_uniform',\n",
    "                                       name='model1_lstm')(model1_embedding)\n",
    "    \n",
    "    model1_dense = tf.keras.layers.Dense(units=vocab_size)(model1_lstm)\n",
    "    \n",
    "    final_model = tf.keras.Model(inputs=model1_input, outputs=model1_dense)\n",
    "                 \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Text Generation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \"\"\"\n",
    "    Generate text by sampling from the model output distribution\n",
    "    as in From https://www.tensorflow.org/tutorials/sequences/text_generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_words = [word.lower() for word in start_string.split(' ')]\n",
    "\n",
    "    num_generate = 50\n",
    "    input_eval = [word2idx[word] for word in start_words]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2word[predicted_id])\n",
    "\n",
    "    return (' '.join(start_words) + ' '.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or Build the Model\n",
    "Text generation requires a batch_size=1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you todayperl popup conventions removes shortcuts signals di messages single publicly although objective realise persistent 27 wrong tries letter somebody webkit look importantly at case template buggy must beginning watching greatly fancy persons illustrate camera 10 postgresql reused homepage param vertex numbers what misunderstood signing we've succeeds what collect tries aggregation\n"
     ]
    }
   ],
   "source": [
    "keras_model_batch1 = build_model(batch_size=1, vocab_size=VOCAB_SIZE, seq_length=MAX_SEQ_LENGTH)\n",
    "print(generate_text(keras_model_batch1, \"How are you today\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Lists to Track Loss and Accuracy at Each Training Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracy = []\n",
    "val_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Evaluation Function to Use During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_evaluate(keras_model, state, val_dataset):\n",
    "    \n",
    "    tff.learning.assign_weights_to_keras_model(keras_model, state.model)\n",
    "    loss, accuracy = keras_model.evaluate(val_dataset, steps=2)\n",
    "    \n",
    "    val_loss.append(loss)\n",
    "    val_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Function and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenedCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n",
    "\n",
    "    def __init__(self, name='accuracy', dtype=None):\n",
    "        super(FlattenedCategoricalAccuracy, self).__init__(name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \n",
    "        y_true = tf.reshape(y_true, [-1, 1])\n",
    "        y_pred = tf.reshape(y_pred, [-1, VOCAB_SIZE, 1])\n",
    "        \n",
    "        return super(FlattenedCategoricalAccuracy, self).update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile(keras_model):\n",
    "    \n",
    "    keras_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[FlattenedCategoricalAccuracy()]\n",
    "    )\n",
    "    \n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Compile the Model\n",
    "The keras model will be accessed as a global variable to create a copy to be called by TFF and will be updated within the training loop to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7eff48225890>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model = build_model(batch_size=BATCH_SIZE,\n",
    "                          vocab_size=VOCAB_SIZE,\n",
    "                          seq_length=MAX_SEQ_LENGTH)\n",
    "compile(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model1_input (InputLayer)    [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "model1_embedding (Embedding) (None, 100, 256)          1281024   \n",
      "_________________________________________________________________\n",
      "model1_lstm (LSTM)           (None, 100, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 5004)         2567052   \n",
      "=================================================================\n",
      "Total params: 5,422,988\n",
      "Trainable params: 5,422,988\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFF Version of the Model to be Trained with Federated Averaging\n",
    "- Clone the keras_model inside `create_tff_model()`, which TFF will call to produce a new copy of the model inside the graph that it will serialize.\n",
    "- TFF uses a `dummy_batch` so it knows the types and shapes that your model expects.\n",
    "- Build and serialize the Tensorflow graph with `build_federated_averaging_process`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tff_model():\n",
    "    \n",
    "    x = tf.constant(np.random.randint(1, VOCAB_SIZE, size=[BATCH_SIZE, MAX_SEQ_LENGTH]))\n",
    "    dummy_batch = collections.OrderedDict([('x', x), ('y', x)]) \n",
    "    keras_model_clone = compile(tf.keras.models.clone_model(keras_model))\n",
    "    \n",
    "    return tff.learning.from_compiled_keras_model(keras_model_clone, dummy_batch=dummy_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/joel_stremmel/anaconda3/envs/tff/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "fed_avg = tff.learning.build_federated_averaging_process(model_fn=create_tff_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Federated Averaging Process and the Starting Model State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If the statement below fails, it means that you are\n",
    "# using an older version of TFF without the high-performance\n",
    "# executor stack. Call `tff.framework.set_default_executor()`\n",
    "# instead to use the default reference runtime.\n",
    "if six.PY3:\n",
    "    tff.framework.set_default_executor(tff.framework.create_local_executor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The state of the FL server, containing the model and optimization state.\n",
    "state = fed_avg.initialize()\n",
    "\n",
    "state = tff.learning.state_with_new_model_weights(\n",
    "    state,\n",
    "    trainable_weights=[v.numpy() for v in keras_model.trainable_weights],\n",
    "    non_trainable_weights=[v.numpy() for v in keras_model.non_trainable_weights]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Create Training Datsets from Randomly Sampled Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_clients(dataset, num_clients):\n",
    "    \n",
    "    random_indices = np.random.choice(len(dataset.client_ids), size=num_clients, replace=False)\n",
    "    \n",
    "    return np.array(dataset.client_ids)[random_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model Across Many Randomly Sampled Clients with Federated Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 10 new clients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel_stremmel/anaconda3/envs/tff/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying federated training round.\n",
      "Training metrics - loss: 7.0183; accuracy: 0.7115\n",
      "Sampling 10 new clients.\n",
      "Applying federated training round.\n",
      "Training metrics - loss: 1.6740; accuracy: 0.8480\n",
      "Sampling 10 new clients.\n",
      "Applying federated training round.\n",
      "Training metrics - loss: 1.4121; accuracy: 0.8418\n",
      "Sampling 10 new clients.\n",
      "Applying federated training round.\n",
      "Training metrics - loss: 1.1855; accuracy: 0.8563\n",
      "Sampling 10 new clients.\n",
      "Applying federated training round.\n",
      "Training metrics - loss: 1.2930; accuracy: 0.8414\n",
      "Sampling 10 new clients.\n",
      "Applying federated training round.\n",
      "Training metrics - loss: 1.1104; accuracy: 0.8367\n",
      "Sampling 10 new clients.\n",
      "Applying federated training round.\n",
      "Training metrics - loss: 1.2125; accuracy: 0.8303\n",
      "Sampling 10 new clients.\n",
      "Applying federated training round.\n",
      "Training metrics - loss: 1.0321; accuracy: 0.8385\n",
      "Sampling 10 new clients.\n",
      "Applying federated training round.\n",
      "Training metrics - loss: 0.9202; accuracy: 0.8582\n",
      "Sampling 10 new clients.\n",
      "Applying federated training round.\n",
      "Training metrics - loss: 1.0141; accuracy: 0.8389\n"
     ]
    }
   ],
   "source": [
    "for round_num in range(NUM_ROUNDS):\n",
    "    \n",
    "#     # Examine validation metrics\n",
    "#     print(f'Evaluating before training round #{round_num} on {NUM_VALIDATION_EXAMPLES} clients.')\n",
    "#     keras_evaluate(keras_model, state, val_data)\n",
    "    \n",
    "    # Sample train clients to create a train dataset\n",
    "    print(f'Sampling {NUM_TRAIN_CLIENTS} new clients.')\n",
    "    train_clients = get_sample_clients(train_data, num_clients=NUM_TRAIN_CLIENTS)\n",
    "    train_datasets = [train_data.create_tf_dataset_for_client(client) for client in train_clients]\n",
    "    \n",
    "    # Apply federated training round\n",
    "    print('Applying federated training round.')\n",
    "    state, metrics = fed_avg.next(state, train_datasets)\n",
    "    \n",
    "    # Examine training metrics\n",
    "    print(f'Training metrics - loss: {metrics[1]:4.4f}; accuracy: {metrics[0]:4.4f}')\n",
    "    train_loss.append(metrics[1])\n",
    "    train_accuracy.append(metrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Model Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_axis = range(0, NUM_ROUNDS)\n",
    "ax.plot(x_axis, train_loss, label='Train')\n",
    "ax.plot(x_axis, val_loss, label='Validation')\n",
    "ax.legend(loc='best')\n",
    "plt.ylabel('Value of Objective Function')\n",
    "plt.title('Model Objective Function at Each Training Round')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_axis = range(0, NUM_ROUNDS)\n",
    "ax.plot(x_axis, train_accuracy, label='Train')\n",
    "ax.plot(x_axis, val_accuracy, label='Validation')\n",
    "ax.legend(loc='best')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy at Each Training Round')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_evaluate(keras_model, state, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text\n",
    "Text generation requires batch_size=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_batch1.set_weights([v.numpy() for v in keras_model.weights])\n",
    "print(generate_text(keras_model_batch1, \"How's the water today? \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggested extensions:**\n",
    "\n",
    "- Use \".repeat(NUM_EPOCHS)\" on the client datasets to try multiple epochs of local training (e.g., as in McMahan et. al.). See also Federated Learning for Image Classification which does this.\n",
    "- Change the compile() command to experiment with using different optimization algorithms on the client.\n",
    "- Try the server_optimizer argument to build_federated_averaging_process to try different algorithms for applying the model updates on the server.\n",
    "- Try the client_weight_fn argument to to build_federated_averaging_process to try different weightings of the clients. The default weights client updates by the number of examples on the client, but you can do e.g. client_weight_fn=lambda _: tf.constant(1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed",
   "language": "python",
   "name": "fed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
