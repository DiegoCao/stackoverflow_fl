{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Overflow Data EDA and Text Generation \n",
    "- Joel Stremmel\n",
    "- 01-14-20\n",
    "\n",
    "**About:**\n",
    "\n",
    "This notebook builds and trains an encoder decoder RNN for text generation with the Stack Overflow data available through `tff.simulation.datasets` with Federared Averaging by following the Federated Learning for Text Generation example notebook listed in the references section.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "This notebook builds and trains an encoder decoder RNN for text generation.\n",
    "\n",
    "\n",
    "**Data:** \n",
    "- https://www.kaggle.com/stackoverflow/stackoverflow\n",
    "\n",
    "**License:** \n",
    "- https://creativecommons.org/licenses/by-sa/3.0/\n",
    "\n",
    "**Data and Model References:**\n",
    "- https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "- https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data\n",
    "- https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb\n",
    "- https://github.com/tensorflow/federated/\n",
    "- https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "**Environment Setup References:**\n",
    "- https://www.tensorflow.org/install/gpu\n",
    "- https://gist.github.com/matheustguimaraes/43e0b65aa534db4df2918f835b9b361d\n",
    "- https://www.tensorflow.org/install/source#tested_build_configurations\n",
    "- https://anbasile.github.io/programming/2017/06/25/jupyter-venv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade tensorflow-federated\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip install --upgrade tensorflow-gpu==2.0\n",
    "# !pip install --upgrade nltk\n",
    "# !pip install matplotlib\n",
    "# !pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import os\n",
    "import six\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built with Cuda: True\n",
      "Build with GPU support: True\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Built with Cuda: {}'.format(tf.test.is_built_with_cuda()))\n",
    "print('Build with GPU support: {}'.format(tf.test.is_built_with_gpu_support()))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Tensorflow to Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices(device_type=None)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[-1], enable=True)\n",
    "for device in physical_devices:\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stack Overflow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel_stremmel/anaconda3/envs/tff/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = tff.simulation.datasets.stackoverflow.load_data(cache_dir='~/data')\n",
    "# print(train_data.output_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Number of Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342477 train clients.\n",
      "38758 val clients.\n",
      "204088 test clients.\n"
     ]
    }
   ],
   "source": [
    "print('{} train clients.'.format(len(train_data.client_ids)))\n",
    "print('{} val clients.'.format(len(val_data.client_ids)))\n",
    "print('{} test clients.'.format(len(test_data.client_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sample Clients for Training, Validation, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_CLIENTS = 50\n",
    "NUM_VAL_CLIENTS = 10\n",
    "NUM_TEST_CLIENTS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_clients(dataset, num_clients):\n",
    "    return np.array(dataset.client_ids)[np.random.choice(len(dataset.client_ids),\n",
    "                                                         size=num_clients,\n",
    "                                                         replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clients = get_sample_clients(train_data, num_clients=NUM_TRAIN_CLIENTS)\n",
    "val_clients = get_sample_clients(val_data, num_clients=NUM_VAL_CLIENTS)\n",
    "test_clients = get_sample_clients(test_data, num_clients=NUM_TEST_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Vocabulary\n",
    "- Currently using the fixed vocabularly of ASCII chars that occur in the works of Shakespeare and Dickens\n",
    "- **Is there a good way to get the distinct characters from a TF dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list('dhlptx@DHLPTX $(,048cgkoswCGKOSW[_#\\'/37;?bfjnrvzBFJNRVZ\"&*.26:\\naeimquyAEIMQUY]!%)-159\\r')\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Mapping from Unique Characters to Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an Encode Decoder RNN\n",
    "Text generation requires a batch_size=1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(batch_size, vocab_size, embedding_dim=256, rnn_units=512):\n",
    "    \"\"\"\n",
    "    Build model with architecture from: https://www.tensorflow.org/tutorials/text/text_generation.\n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        \n",
    "        tf.keras.layers.Embedding(vocab_size,\n",
    "                                  embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        \n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True, \n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "                                 \n",
    "#         tf.keras.layers.LSTM(rnn_units,\n",
    "#                              return_sequences=True,\n",
    "#                              stateful=True,\n",
    "#                              recurrent_initializer='glorot_uniform'),\n",
    "                                 \n",
    "        tf.keras.layers.Dense(vocab_size)])\n",
    "                 \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        \n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        \n",
    "    return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        \n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "    \n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \"\"\"\n",
    "    Generate text by sampling from the model output distribution\n",
    "    as in From https://www.tensorflow.org/tutorials/sequences/text_generation.\n",
    "    \"\"\"\n",
    "\n",
    "    num_generate = 200\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keras_model_batch1 = load_pretrained_model(batch_size=1)\n",
    "keras_model_batch1 = build_model(batch_size=1, vocab_size=vocab_size)\n",
    "print(generate_text(keras_model_batch1, 'What of TensorFlow Federated, you ask? '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Federated Stack Overflow\n",
    "- Using a namedtuple with keys x and y as the output type of the dataset keeps both TFF and Keras happy.\n",
    "- Construct a lookup table to map string chars to indexes, using the vocab loaded above.\n",
    "- Write functions for:\n",
    "    - ID lookup\n",
    "    - Splitting inputs and targets\n",
    "    - Applying preprocessing steps to dataset\n",
    "    - Taking clients and client records and applying preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 100\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchType = collections.namedtuple('BatchType', ['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=vocab,\n",
    "        values=tf.constant(list(range(len(vocab))),\n",
    "        dtype=tf.int64)),\n",
    "    default_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ids(x):\n",
    "    \n",
    "    s = tf.reshape(x['tokens'], shape=[1])\n",
    "    chars = tf.strings.bytes_split(s).values\n",
    "    ids = table.lookup(chars)\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    \n",
    "    input_text = tf.map_fn(lambda x: x[:-1], chunk)\n",
    "    target_text = tf.map_fn(lambda x: x[1:], chunk)\n",
    "    \n",
    "    return BatchType(input_text, target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    \n",
    "    return (\n",
    "        # Map ASCII chars to int64 indexes using the vocab\n",
    "        dataset.map(to_ids)\n",
    "        # Split into individual chars\n",
    "        .unbatch()\n",
    "        # Form example sequences of SEQ_LENGTH +1\n",
    "        .batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "        # Shuffle and form minibatches\n",
    "        .shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "        # And finally split into (input, target) tuples,\n",
    "        # each of length SEQ_LENGTH.\n",
    "        .map(split_input_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_for_client(client, source=train_data):\n",
    "    \n",
    "    return preprocess(source.create_tf_dataset_for_client(client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = [preprocess_data_for_client(client, train_data) for client in train_clients]\n",
    "print(tf.data.experimental.get_structure(train_datasets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Test on Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenedCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n",
    "\n",
    "    def __init__(self, name='accuracy', dtype=None):\n",
    "        super(FlattenedCategoricalAccuracy, self).__init__(name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \n",
    "        y_true = tf.reshape(y_true, [-1, 1])\n",
    "        y_pred = tf.reshape(y_pred, [-1, len(vocab), 1])\n",
    "        \n",
    "        return super(FlattenedCategoricalAccuracy, self).update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred, loss_objective):\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_objective(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile(keras_model, loss_objective):\n",
    "    \n",
    "    keras_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr=0.005), # updated from SGD; TO DO: experiment with a scheduler\n",
    "        loss=loss_objective,\n",
    "        metrics=[FlattenedCategoricalAccuracy()]\n",
    "    )\n",
    "    \n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_objective = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = build_model(batch_size=BATCH_SIZE, vocab_size=vocab_size)\n",
    "compile(keras_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve Model with Federated Averaging\n",
    "- Clone the keras_model inside `create_tff_model()`, which TFF will call to produce a new copy of the model inside the graph that it will serialize.\n",
    "- TFF uses a `dummy_batch` so it knows the types and shapes that your model expects.\n",
    "- Build and serialize the Tensorflow graph with `build_federated_averaging_process`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tff_model():\n",
    "    \n",
    "    x = tf.constant(np.random.randint(1, len(vocab), size=[BATCH_SIZE, SEQ_LENGTH]))\n",
    "    dummy_batch = collections.OrderedDict([('x', x), ('y', x)]) \n",
    "    keras_model_clone = compile(tf.keras.models.clone_model(keras_model))\n",
    "    \n",
    "    return tff.learning.from_compiled_keras_model(keras_model_clone, dummy_batch=dummy_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_avg = tff.learning.build_federated_averaging_process(model_fn=create_tff_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run One Round of Federated Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = fed_avg.initialize()\n",
    "state, metrics = fed_avg.next(state, [train_datasets[0].take(1)])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Preprocess the Validation and Test Datasets\n",
    "Concatenate the validation and test datasets for evaluation with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = functools.reduce(lambda d1, d2: d1.concatenate(d2), \n",
    "                               [preprocess_data_for_client(client, val_data) for client in val_clients])\n",
    "\n",
    "test_dataset = functools.reduce(lambda d1, d2: d1.concatenate(d2), \n",
    "                                [preprocess_data_for_client(client, test_data) for client in test_clients])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Federated Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If the statement below fails, it means that you are\n",
    "# using an older version of TFF without the high-performance\n",
    "# executor stack. Call `tff.framework.set_default_executor()`\n",
    "# instead to use the default reference runtime.\n",
    "if six.PY3:\n",
    "    tff.framework.set_default_executor(tff.framework.create_local_executor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROUNDS = 20\n",
    "\n",
    "# The state of the FL server, containing the model and optimization state.\n",
    "state = fed_avg.initialize()\n",
    "\n",
    "state = tff.learning.state_with_new_model_weights(\n",
    "    state,\n",
    "    trainable_weights=[v.numpy() for v in keras_model.trainable_weights],\n",
    "    non_trainable_weights=[v.numpy() for v in keras_model.non_trainable_weights]\n",
    ")\n",
    "\n",
    "def keras_evaluate(state, round_num):\n",
    "    tff.learning.assign_weights_to_keras_model(keras_model, state.model)\n",
    "    print('Evaluating before training round', round_num)\n",
    "    keras_model.evaluate(val_dataset, steps=2)\n",
    "\n",
    "for round_num in range(NUM_ROUNDS):\n",
    "    keras_evaluate(state, round_num)\n",
    "    state, metrics = fed_avg.next(state, train_datasets)\n",
    "    print('Training metrics: ', metrics)\n",
    "\n",
    "keras_evaluate(state, NUM_ROUNDS + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text\n",
    "Text generation requires batch_size=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_batch1.set_weights([v.numpy() for v in keras_model.weights])\n",
    "print(generate_text(keras_model_batch1, 'What of TensorFlow Federated, you ask?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggested extensions from tutorial:**\n",
    "\n",
    "- Write a more realistic training loop where you sample clients to train on randomly.\n",
    "- Use \".repeat(NUM_EPOCHS)\" on the client datasets to try multiple epochs of local training (e.g., as in McMahan et. al.). See also Federated Learning for Image Classification which does this.\n",
    "- Change the compile() command to experiment with using different optimization algorithms on the client.\n",
    "- Try the server_optimizer argument to build_federated_averaging_process to try different algorithms for applying the model updates on the server.\n",
    "- Try the client_weight_fn argument to to build_federated_averaging_process to try different weightings of the clients. The default weights client updates by the number of examples on the client, but you can do e.g. client_weight_fn=lambda _: tf.constant(1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed",
   "language": "python",
   "name": "fed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
