{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Level Federated Text Generation with Stack Overflow with Randomly Initialized or Pretrained Word Embeddings\n",
    "- Joel Stremmel\n",
    "- 02-19-20\n",
    "- Runs on GCP and local Ubuntu 16.04\n",
    "\n",
    "**About:**\n",
    "\n",
    "This notebook loads the Stack Overflow data available through `tff.simulation.datasets` and trains an LSTM model with Federared Averaging by following the Federated Learning for Text Generation [example notebook](https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb).  The embedding layer is optionally initialized with [GloVe](https://nlp.stanford.edu/projects/glove/) ([license here](https://www.opendatacommons.org/licenses/pddl/1.0/)), [FastText](https://fasttext.cc/docs/en/english-vectors.html) ([license here](https://creativecommons.org/licenses/by-sa/3.0/)), [GPT-2](https://openai.com/blog/better-language-models/) ([license here](https://github.com/huggingface/transformers/blob/master/LICENSE)), or randomly initialized embeddings.  After downloading the GloVe or FastText embeddings, place the embedding files in a directory called `word_embedding` at the top level of the repository.  GPT-2 embeddings are downloaded by running the notebook which makes a call to `src/embeddings.py` to download the embeddings from HuggingFace.  For more information on downloading or using these embeddings, see the research report.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- This notebook prepares the Stack Overflow dataset for word level language modeling using this [module](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/dataset.py\n",
    ").\n",
    "- The metrics for model training come from this [module](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/metrics.py). \n",
    "\n",
    "\n",
    "**Data and Model References:**\n",
    "- [TFF Stack Overflow `load_data`](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data)\n",
    "- [TFF text generation tutorial](https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb)\n",
    "- [Google TFF team research baselines for Stack Overflow](https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/baselines/stackoverflow)\n",
    "- [Tensorflow text generation tutorial](https://www.tensorflow.org/tutorials/text/text_generation)\n",
    "\n",
    "**Environment Setup References:**\n",
    "- [Installing Tensorflow for GPU](https://www.tensorflow.org/install/gpu)\n",
    "- [Install CUDA 10.0 and cuDNN v7.4.2 on Ubuntu 16.04](https://gist.github.com/matheustguimaraes/43e0b65aa534db4df2918f835b9b361d)\n",
    "- [Tensorflow build configs](https://www.tensorflow.org/install/source#tested_build_configurations)\n",
    "- [Using jupyter notebooks with a virtual environment](https://anbasile.github.io/programming/2017/06/25/jupyter-venv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, io\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import six\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import dataset, metrics, embeddings, model, validation, federated, generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2idx = {'hello': 0, 'my': 1, 'darling': 2, 'gobdobdob': 3}\n",
    "# embedding_index = embeddings.create_gpt_embeddings(word2idx)\n",
    "# reduced_embedding_index = embeddings.to_pca_projection(embedding_index, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Compatability Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Tensorflow Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Built with Cuda: {}'.format(tf.test.is_built_with_cuda()))\n",
    "print('Build with GPU support: {}'.format(tf.test.is_built_with_gpu_support()))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Some Parameters for Preprocessing the Data and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "CLIENTS_EPOCHS_PER_ROUND = 1\n",
    "MAX_SEQ_LENGTH = 20\n",
    "MAX_ELEMENTS_PER_USER = 5000\n",
    "CENTRALIZED_TRAIN = False\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "NUM_VALIDATION_EXAMPLES = 10000\n",
    "NUM_TEST_EXAMPLES = 2\n",
    "\n",
    "NUM_ROUNDS = 10\n",
    "NUM_TRAIN_CLIENTS = 10\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "RNN_UNITS = 256\n",
    "\n",
    "EMBEDDING_LAYER = 'glove'\n",
    "SAV = 'embedding_layer_results/{}_{}_{}_{}/'.format(EMBEDDING_LAYER, \n",
    "                                                    EMBEDDING_DIM, \n",
    "                                                    RNN_UNITS, \n",
    "                                                    EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Output Directory if it Nonexistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAV):\n",
    "    os.makedirs(SAV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Word Level Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = dataset.construct_word_level_datasets(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    client_epochs_per_round=CLIENTS_EPOCHS_PER_ROUND,\n",
    "    max_seq_len=MAX_SEQ_LENGTH,\n",
    "    max_elements_per_user=MAX_ELEMENTS_PER_USER,\n",
    "    centralized_train=CENTRALIZED_TRAIN,\n",
    "    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
    "    num_validation_examples=NUM_VALIDATION_EXAMPLES,\n",
    "    num_test_examples=NUM_TEST_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Dataset Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dataset.get_vocab(vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Special Characters Created During Preprocessing\n",
    "The four special tokens are:\n",
    "- pad: padding token\n",
    "- oov: out of vocabulary\n",
    "- bos: begin of sentence\n",
    "- eos: end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = list(dataset.get_special_tokens(vocab_size=VOCAB_SIZE))\n",
    "special_words = list(dataset.get_special_token_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special2idx = dict(zip(special_tokens, special_words))\n",
    "idx2special = {v:k for k, v in special2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Vocabulary\n",
    "Add one to account for the pad token which has idx 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word:i+1 for i, word in enumerate(vocab)}\n",
    "idx2word = {i+1:word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {**word2idx, **special2idx}\n",
    "idx2word = {**idx2word, **idx2special}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Vocab Size\n",
    "This accounts for having added the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENDED_VOCAB_SIZE = VOCAB_SIZE + len(special2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Word Embeddings\n",
    "- Either [GloVe embeddings from Stanford](https://nlp.stanford.edu/projects/glove/) - license [here](https://www.opendatacommons.org/licenses/pddl/1.0/)\n",
    "- [FastText embeddings from Facebook](https://fasttext.cc/docs/en/english-vectors.html) - license [here](https://creativecommons.org/licenses/by-sa/3.0/)\n",
    "- Or word embeddings from [OpenAI GPT-2](https://openai.com/blog/better-language-models/) - license [here](https://github.com/huggingface/transformers/blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embedding Matrix from Words in Word Index and Word Embeddings\n",
    "If the `EMBEDDING_LAYER` option is set to 'random', the embedding matrix in the embedding layer is initialized according to the random uniform distribution used by the tf.keras embedding layer by passing the 'uniform' string as an argument to the `embedding_initializer` in the `build_model` function.  Otherwise, an embedding index called `word2embedding` is created from pretrained embeddings either loaded from the 'word_embeddings' directory or created from a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDING_LAYER == 'random':\n",
    "    pass\n",
    "\n",
    "elif EMBEDDING_LAYER == 'glove':\n",
    "    embedding_path = '../word_embeddings/glove/glove.6B.{}d.txt'.format(EMBEDDING_DIM)\n",
    "    word2embedding = embeddings.load_embeddings(embedding_path)\n",
    "    \n",
    "elif EMBEDDING_LAYER == 'fasttext':\n",
    "    embedding_path = '../word_embeddings/fasttext/wiki-news-{}d-1M.vec'.format(EMBEDDING_DIM)\n",
    "    word2embedding = embeddings.load_embeddings(embedding_path)\n",
    "    \n",
    "elif EMBEDDING_LAYER == 'projected_gpt2':\n",
    "    word2embedding = embeddings.create_gpt_embeddings(word2idx)\n",
    "    word2embedding = embeddings.to_pca_projections(word2embedding, n=2)\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"EMBEDDING LAYER must be one of 'random', 'glove', 'fasttext', or 'projected_gpt2'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDING_LAYER == 'random':\n",
    "    embedding_matrix = 'uniform'\n",
    "else:\n",
    "    embedding_matrix = embeddings.create_matrix_from_pretrained_embeddings(\n",
    "        word2embedding=word2embedding,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = model.build_model(extended_vocab_size=EXTENDED_VOCAB_SIZE,\n",
    "                                embedding_dim=EMBEDDING_DIM,\n",
    "                                embedding_matrix=embedding_matrix,\n",
    "                                rnn_units=RNN_UNITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or Build the Model and Try Generating Some Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = model.build_model(extended_vocab_size=EXTENDED_VOCAB_SIZE,\n",
    "                                embedding_dim=EMBEDDING_DIM,\n",
    "                                embedding_matrix=embedding_matrix,\n",
    "                                rnn_units=RNN_UNITS)\n",
    "\n",
    "generate_text.generate_text(model=keras_model,\n",
    "                            word2idx=word2idx,\n",
    "                            idx2word=idx2word,\n",
    "                            start_string='How are you today')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFF Version of the Model to be Trained with Federated Averaging\n",
    "- TFF uses a sample batch so it knows the types and shapes that your model expects.\n",
    "- The model function then builds and compiles the model and creates a TFF version to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(val_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Train and Validation Model Trackers to be Used Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metric_names = ['loss',\n",
    "                           'num_tokens',\n",
    "                           'num_tokens_no_oov',\n",
    "                           'num_batches',\n",
    "                           'num_examples',\n",
    "                           'accuracy',\n",
    "                           'accuracy_no_oov',\n",
    "                           'accuracy_no_oov_no_oes']\n",
    "\n",
    "train_metrics_tracker = validation.model_history_tracker(metric_names=evaluation_metric_names)\n",
    "val_metrics_tracker = validation.model_history_tracker(metric_names=evaluation_metric_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Default Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff.framework.set_default_executor(tff.framework.local_executor_factory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Iterative Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_process = (\n",
    "      tff.learning.federated_averaging.build_federated_averaging_process(\n",
    "          model_fn=lambda : model.model_fn(extended_vocab_size=EXTENDED_VOCAB_SIZE,\n",
    "                        embedding_dim=EMBEDDING_DIM,\n",
    "                        embedding_matrix=embedding_matrix,\n",
    "                        rnn_units=RNN_UNITS,\n",
    "                        vocab_size=VOCAB_SIZE,\n",
    "                        sample_batch=sample_batch),\n",
    "          server_optimizer_fn=federated.server_optimizer_fn,\n",
    "          client_weight_fn=federated.client_weight_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initalize the Process\n",
    "Server state will be updated in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model Across Many Randomly Sampled Clients with Federated Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for round_num in range(0, NUM_ROUNDS):\n",
    "\n",
    "        # Examine validation metrics\n",
    "        print('Evaluating before training round #{} on {} examples.'.format(round_num, NUM_VALIDATION_EXAMPLES))\n",
    "        validation.keras_evaluate(state=server_state,\n",
    "                                  val_dataset=val_data,\n",
    "                                  extended_vocab_size=EXTENDED_VOCAB_SIZE,\n",
    "                                  vocab_size=VOCAB_SIZE,\n",
    "                                  embedding_dim=EMBEDDING_DIM,\n",
    "                                  embedding_matrix=embedding_matrix,\n",
    "                                  rnn_units=RNN_UNITS,\n",
    "                                  metrics_tracker=val_metrics_tracker)\n",
    "\n",
    "        # Sample train clients to create a train dataset\n",
    "        print('Sampling {} new clients.'.format(NUM_TRAIN_CLIENTS))\n",
    "        train_clients = federated.get_sample_clients(dataset=train_data, num_clients=NUM_TRAIN_CLIENTS)\n",
    "        train_datasets = [train_data.create_tf_dataset_for_client(client) for client in train_clients]\n",
    "\n",
    "        # Apply federated training round\n",
    "        server_state, server_metrics = iterative_process.next(server_state, train_datasets)\n",
    "\n",
    "        # Examine training metrics\n",
    "        print('Round: {}'.format(round_num))\n",
    "        print('   Loss: {:.8f}'.format(server_metrics.loss))\n",
    "        print('   num_batches: {}'.format(server_metrics.num_batches))\n",
    "        print('   num_examples: {}'.format(server_metrics.num_examples))\n",
    "        print('   num_tokens: {}'.format(server_metrics.num_tokens))\n",
    "        print('   num_tokens_no_oov: {}'.format(server_metrics.num_tokens_no_oov))\n",
    "        print('   accuracy: {:.5f}'.format(server_metrics.accuracy))\n",
    "        print('   accuracy_no_oov: {:.5f}'.format(server_metrics.accuracy_no_oov))\n",
    "\n",
    "        # Add train metrics to tracker\n",
    "        train_metrics_tracker.add_metrics_by_name('loss', server_metrics.loss)\n",
    "        train_metrics_tracker.add_metrics_by_name('accuracy', server_metrics.accuracy)\n",
    "        train_metrics_tracker.add_metrics_by_name('num_examples', server_metrics.num_examples)\n",
    "        train_metrics_tracker.add_metrics_by_name('num_tokens', server_metrics.num_tokens)\n",
    "        train_metrics_tracker.add_metrics_by_name('num_tokens_no_oov', server_metrics.num_tokens_no_oov)\n",
    "        \n",
    "        # Save loss and accuracy from train and validation sets\n",
    "        np.save(SAV + 'train_loss.npy', train_metrics_tracker.get_metrics_by_name('loss'))\n",
    "        np.save(SAV + 'val_loss.npy', val_metrics_tracker.get_metrics_by_name('loss'))\n",
    "        np.save(SAV + 'train_accuracy.npy', train_metrics_tracker.get_metrics_by_name('accuracy'))\n",
    "        np.save(SAV + 'val_accuracy.npy', val_metrics_tracker.get_metrics_by_name('accuracy'))\n",
    "        \n",
    "        # Save train sample stats\n",
    "        np.save(SAV + 'num_examples.npy', train_metrics_tracker.get_metrics_by_name('num_examples'))\n",
    "        np.save(SAV + 'num_tokens.npy', train_metrics_tracker.get_metrics_by_name('num_tokens'))\n",
    "        np.save(SAV + 'num_tokens_no_oov.npy', train_metrics_tracker.get_metrics_by_name('num_tokens_no_oov'))\n",
    "        \n",
    "except KeyboardInterrupt as ke:\n",
    "    \n",
    "    print('Interrupted')\n",
    "    \n",
    "except:\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 15))\n",
    "    x_axis = range(0, len(train_metrics_tracker.get_metrics_by_name('loss')))\n",
    "    ax.plot(x_axis, train_metrics_tracker.get_metrics_by_name('loss'), label='Train')\n",
    "    ax.plot(x_axis, val_metrics_tracker.get_metrics_by_name('loss'), label='Val')\n",
    "    ax.legend(loc='best', prop={'size': 15})\n",
    "    plt.title('Loss by Epoch', fontsize=20)\n",
    "    plt.xlabel('Epochs', fontsize=18)\n",
    "    plt.ylabel('Loss', fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(SAV + 'Loss by Epoch.png')\n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 15))\n",
    "    ax.plot(x_axis, train_metrics_tracker.get_metrics_by_name('accuracy'), label='Train')\n",
    "    ax.plot(x_axis, val_metrics_tracker.get_metrics_by_name('accuracy'), label='Val')\n",
    "    ax.legend(loc='best', prop={'size': 15})\n",
    "    plt.title('Accuracy by Epoch', fontsize=20)\n",
    "    plt.xlabel('Epochs', fontsize=18)\n",
    "    plt.ylabel('Accuracy', fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(SAV + 'Accuracy by Epoch.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train Sample Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = np.load(SAV + 'num_examples.npy')\n",
    "tokens = np.load(SAV + 'num_tokens.npy')\n",
    "tokens_no_oov = np.load(SAV + 'num_tokens_no_oov.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Train Sample Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_config = 'Clients: {}, Max Elements per Client: {}, Max Seq Len: {}, Num Rounds: {}'.format(\n",
    "    NUM_TRAIN_CLIENTS, MAX_ELEMENTS_PER_USER, MAX_SEQ_LENGTH, NUM_ROUNDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_stats = ['Examples', 'Tokens', 'Tokens No OOV']\n",
    "means = [np.mean(examples), np.mean(tokens), np.mean(tokens_no_oov)]\n",
    "stdvs = [np.std(examples), np.std(tokens), np.std(tokens_no_oov)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "x_pos = np.arange(len(train_sample_stats))\n",
    "ax.bar(x_pos, means, yerr=stdvs, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_ylabel('Sample Mean +- 1 Stdv')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(train_sample_stats)\n",
    "ax.set_title('Train Sample Means - {}'.format(round_config))\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAV + '{} Round Train Sample Means.png'.format(NUM_ROUNDS))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.hist(examples, alpha=0.4, label='Num Examples')\n",
    "plt.hist(tokens, alpha=0.4, label='Num Tokens')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Train Sample Distributions - {}'.format(round_config))\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAV + '{} Round Train Sample Distributions.png'.format(NUM_ROUNDS))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.hist(examples, alpha=0.8, label='Num Examples')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Train Sample Distribution - {}'.format(round_config))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.hist(tokens, alpha=0.8, label='Num Tokens')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Train Sample Distribution - {}'.format(round_config))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.hist(tokens_no_oov, alpha=0.8, label='Num Tokens No OOV')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Train Sample Distribution - {}'.format(round_config))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed",
   "language": "python",
   "name": "fed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
