{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Level Federated Text Generation with Stack Overflow with and without Pretrained Word Embeddings\n",
    "- 02-07-20\n",
    "\n",
    "**About:**\n",
    "\n",
    "This notebook loads the Stack Overflow data available through `tff.simulation.datasets` and trains an LSTM model with Federared Averaging by following the Federated Learning for Text Generation [example notebook](https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb).  The embedding layer in the LSTM is optionally initialized with either GloVe or FastText Embeddings.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- This notebook prepares the Stack Overflow dataset for word level language modeling using this [module](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/dataset.py\n",
    ").\n",
    "- The metrics for model training come from this [module](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/metrics.py). \n",
    "\n",
    "\n",
    "**Data:** \n",
    "- https://www.kaggle.com/stackoverflow/stackoverflow\n",
    "\n",
    "**License:** \n",
    "- https://creativecommons.org/licenses/by-sa/3.0/\n",
    "\n",
    "**Data and Model References:**\n",
    "- https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data\n",
    "- https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb\n",
    "- https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/baselines/stackoverflow\n",
    "- https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "**Environment Setup References:**\n",
    "- https://www.tensorflow.org/install/gpu\n",
    "- https://gist.github.com/matheustguimaraes/43e0b65aa534db4df2918f835b9b361d\n",
    "- https://www.tensorflow.org/install/source#tested_build_configurations\n",
    "- https://anbasile.github.io/programming/2017/06/25/jupyter-venv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Pip install these packages in the order listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade tensorflow-federated\n",
    "# !pip tensorflow\n",
    "# !pip install --upgrade tensorflow-gpu==2.0\n",
    "# !pip install --upgrade nltk\n",
    "# !pip install matplotlib\n",
    "# !pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, io\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/dataset.py\n",
    "from src.dataset import construct_word_level_datasets, get_vocab, get_special_tokens\n",
    "\n",
    "# from https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/metrics.py\n",
    "import src.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import six\n",
    "import time\n",
    "import string\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Compatability Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Tensorflow Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built with Cuda: True\n",
      "Build with GPU support: True\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print('Built with Cuda: {}'.format(tf.test.is_built_with_cuda()))\n",
    "print('Build with GPU support: {}'.format(tf.test.is_built_with_gpu_support()))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Tensorflow to Use GPU, unless GPU isn't available (which is the case for me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices(device_type=None)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[-1], enable=True)\n",
    "for device in physical_devices:\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Some Parameters for Preprocessing the Data and Training the Model\n",
    "**Note:** Ask Keith how he's been setting there for internal experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "BATCH_SIZE = 16\n",
    "CLIENTS_EPOCHS_PER_ROUND = 1\n",
    "MAX_SEQ_LENGTH = 20\n",
    "MAX_ELEMENTS_PER_USER = 5000\n",
    "CENTRALIZED_TRAIN = False\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "NUM_VALIDATION_EXAMPLES = 10000\n",
    "NUM_TEST_EXAMPLES = 2\n",
    "\n",
    "NUM_ROUNDS = 10\n",
    "NUM_TRAIN_CLIENTS = 10\n",
    "UNIFORM_WEIGHTING = False\n",
    "\n",
    "USE_PRETRAINED_EMBEDDING = True\n",
    "EMBEDDING_DIM = 300\n",
    "#EMBEDDING_PATH = f'../../../word_embeddings/glove/glove.6B.{EMBEDDING_DIM}d.txt'\n",
    "EMBEDDING_PATH = f'../../../word_embeddings/fasttext/wiki-news-{EMBEDDING_DIM}d-1M.vec'\n",
    "#EMBEDDING_PATH = f'../../../word_embeddings/fasttext/crawl-{EMBEDDING_DIM}d-2M.vec'\n",
    "\n",
    "\n",
    "RNN_UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Word Level Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = construct_word_level_datasets(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    client_epochs_per_round=CLIENTS_EPOCHS_PER_ROUND,\n",
    "    max_seq_len=MAX_SEQ_LENGTH,\n",
    "    max_elements_per_user=MAX_ELEMENTS_PER_USER,\n",
    "    centralized_train=CENTRALIZED_TRAIN,\n",
    "    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
    "    num_validation_examples=NUM_VALIDATION_EXAMPLES,\n",
    "    num_test_examples=NUM_TEST_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Dataset Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Special Characters Created During Preprocessing\n",
    "The four special tokens are:\n",
    "- pad: padding token\n",
    "- oov: out of vocabulary\n",
    "- bos: begin of sentence\n",
    "- eos: end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad, oov, bos, eos = get_special_tokens(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special2idx = dict(zip(['pad', 'oov', 'bos', 'eos'], [pad, oov, bos, eos]))\n",
    "idx2special = {v:k for k, v in special2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Vocabulary\n",
    "Add one to account for the pad token which has idx 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word:i+1 for i, word in enumerate(vocab)}\n",
    "idx2word = {i+1:word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {**word2idx, **special2idx}\n",
    "idx2word = {**idx2word, **idx2special}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Vocab Size\n",
    "This accounts for having added the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENDED_VOCAB_SIZE = VOCAB_SIZE + len(special2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Word Embeddings\n",
    "- Either [GloVe embeddings from Stanford](https://nlp.stanford.edu/projects/glove/) - license [here](https://www.opendatacommons.org/licenses/pddl/1.0/)\n",
    "- Or [FastText embeddings from Facebook](https://fasttext.cc/docs/en/english-vectors.html) - license [here](https://creativecommons.org/licenses/by-sa/3.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext_embeddings():\n",
    "    \n",
    "    \"\"\"\n",
    "    Code modified from: https://fasttext.cc/docs/en/english-vectors.html.\n",
    "    \"\"\"\n",
    "    \n",
    "    fin = io.open(EMBEDDING_PATH, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = [int(x) for x in fin.readline().split()]\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        embeddings_index[tokens[0]] = np.array([float(x) for x in tokens[1:]])\n",
    "        \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings():\n",
    "    \n",
    "    \"\"\"\n",
    "    Code modified from: https://keras.io/examples/pretrained_word_embeddings/.\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    with open(EMBEDDING_PATH, 'r') as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "            \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embedding Matrix from Words in Word Index and Word Embeddings\n",
    "Words with no embedding are initialized according to the random uniform distribution used by the tf.keras embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_from_pretrained_embeddings(embedding_fn, word2idx):\n",
    "                                             \n",
    "    \"\"\"\n",
    "    Code modified from: https://keras.io/examples/pretrained_word_embeddings/.\n",
    "    Here we initialize the embedding matrix to random uniform values according\n",
    "    to the random uniform initializer in tf.keras used by tf.keras.layers.Embedding.\n",
    "    This way any words without embeddings are mapped according to this random\n",
    "    distribution and can then be learned.\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings_index = embedding_fn()\n",
    "    \n",
    "    em_init_params = tf.random_uniform_initializer().get_config()\n",
    "    embedding_matrix = np.random.uniform(low=em_init_params['minval'],\n",
    "                                         high=em_init_params['maxval'],\n",
    "                                         size=(EXTENDED_VOCAB_SIZE, EMBEDDING_DIM))\n",
    "    \n",
    "    missing_words = 0\n",
    "    for word, i in word2idx.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            missing_words += 1\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    print(f'Words from dataset with no embedding: {missing_words}')\n",
    "    \n",
    "    return tf.keras.initializers.Constant(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PRETRAINED_EMBEDDING:\n",
    "    EMBEDDING_MATRIX =  create_matrix_from_pretrained_embeddings(embedding_fn=load_fasttext_embeddings,\n",
    "                                                                 word2idx=word2idx)\n",
    "else:\n",
    "    EMBEDDING_MATRIX = 'uniform'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Build model with architecture from: https://www.tensorflow.org/tutorials/text/text_generation.\n",
    "    \"\"\"\n",
    "\n",
    "    model1_input = tf.keras.Input(shape=(None, ),\n",
    "                                  name='model1_input')\n",
    "    \n",
    "    model1_embedding = tf.keras.layers.Embedding(input_dim=EXTENDED_VOCAB_SIZE,\n",
    "                                                 output_dim=EMBEDDING_DIM,\n",
    "                                                 embeddings_initializer=EMBEDDING_MATRIX,\n",
    "                                                 mask_zero=True,\n",
    "                                                 trainable=True,\n",
    "                                                 name='model1_embedding')(model1_input)\n",
    "    \n",
    "    model1_lstm = tf.keras.layers.LSTM(units=RNN_UNITS,\n",
    "                                       return_sequences=True,\n",
    "                                       recurrent_initializer='glorot_uniform',\n",
    "                                       name='model1_lstm')(model1_embedding)\n",
    "    \n",
    "    model1_dense1 = tf.keras.layers.Dense(units=EMBEDDING_DIM)(model1_lstm)\n",
    "    \n",
    "    model1_dense2 = tf.keras.layers.Dense(units=EXTENDED_VOCAB_SIZE)(model1_dense1)\n",
    "    \n",
    "    final_model = tf.keras.Model(inputs=model1_input, outputs=model1_dense2)\n",
    "                 \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Text Generation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \"\"\"\n",
    "    Generate text by sampling from the model output distribution\n",
    "    as in From https://www.tensorflow.org/tutorials/sequences/text_generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_words = [word.lower() for word in start_string.split(' ')]\n",
    "\n",
    "    num_generate = 50\n",
    "    input_eval = [word2idx[word] for word in start_words]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        #input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        input_eval = tf.concat([input_eval, tf.expand_dims([predicted_id], 0)], 1)\n",
    "        text_generated.append(idx2word[predicted_id])\n",
    "\n",
    "    return (' '.join(start_words) + ' ' + ' '.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or Build the Model and Try Generating Some Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = build_model()\n",
    "print(generate_text(keras_model, \"How are you today\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Metrics for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics():\n",
    "    \n",
    "    evaluation_metrics = [\n",
    "        metrics.NumTokensCounter(name='num_tokens', masked_tokens=[pad]),\n",
    "        metrics.NumTokensCounter(name='num_tokens_no_oov', masked_tokens=[pad, oov]),\n",
    "        metrics.NumBatchesCounter(name='num_batches'),\n",
    "        metrics.NumExamplesCounter(name='num_examples'),\n",
    "        metrics.MaskedCategoricalAccuracy(name='accuracy', masked_tokens=[pad]),\n",
    "        metrics.MaskedCategoricalAccuracy(name='accuracy_no_oov', masked_tokens=[pad, oov]),\n",
    "        metrics.MaskedCategoricalAccuracy(name='accuracy_no_oov_no_eos', masked_tokens=[pad, oov, eos])\n",
    "    ]\n",
    "    \n",
    "    return evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Function to Compile the Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(keras_model, evaluation_metrics):\n",
    "    \n",
    "    keras_model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=evaluation_metrics)\n",
    "    \n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFF Version of the Model to be Trained with Federated Averaging\n",
    "- TFF uses a sample batch so it knows the types and shapes that your model expects.\n",
    "- The model function then builds and compiles the model and creates a TFF version to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Create TFF model from compiled Keras model and a sample batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    keras_model = build_model()\n",
    "    evaluation_metrics = get_metrics()\n",
    "    \n",
    "    compile_model(keras_model, evaluation_metrics)\n",
    "    \n",
    "    return tff.learning.from_compiled_keras_model(keras_model, sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model History Tracker to Save Metrics at Each Training Round\n",
    "This is a spin off of [keras' model history](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History) but is specific to the TFF training code used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_history_tracker:\n",
    "    \n",
    "    def __init__(self, metric_names=[]):\n",
    "        \n",
    "        self.metric_names = metric_names\n",
    "        self.metrics_dict = {name:[] for name in metric_names}\n",
    "    \n",
    "    def get_metrics_by_name(self, metric_name):\n",
    "        \n",
    "        return self.metrics_dict[metric_name]\n",
    "    \n",
    "    def add_metrics_by_name(self, metric_name, metric_result):\n",
    "        \n",
    "        self.metrics_dict[metric_name].append(metric_result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Train and Validation Model Trackers to be Used Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metric_names = ['loss',\n",
    "                           'num_tokens',\n",
    "                           'num_tokens_no_oov',\n",
    "                           'num_batches',\n",
    "                           'num_examples',\n",
    "                           'accuracy',\n",
    "                           'accuracy_no_oov',\n",
    "                           'accuracy_no_oov_no_oes']\n",
    "\n",
    "train_metrics_tracker = model_history_tracker(evaluation_metric_names)\n",
    "val_metrics_tracker = model_history_tracker(evaluation_metric_names)\n",
    "\n",
    "train_metrics_tracker_with_embedding = model_history_tracker(evaluation_metric_names)\n",
    "val_metrics_tracker_with_embedding = model_history_tracker(evaluation_metric_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Evaluate Model Performance on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_evaluate(state, val_dataset):\n",
    "    \n",
    "    keras_model = build_model()\n",
    "    evaluation_metrics = get_metrics()\n",
    "    \n",
    "    compile_model(keras_model, evaluation_metrics)\n",
    "    tff.learning.assign_weights_to_keras_model(keras_model, state.model)\n",
    "    \n",
    "    evaluation_results = keras_model.evaluate(val_dataset)\n",
    "    \n",
    "    for i, result in enumerate(evaluation_results):\n",
    "        val_metrics_tracker.add_metrics_by_name(val_metrics_tracker.metric_names[i], result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Weight Clients Uniformly or by Number of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_weight_fn(local_outputs):\n",
    "    \n",
    "    num_tokens = tf.cast(tf.squeeze(local_outputs['num_tokens']), tf.float32)\n",
    "    \n",
    "    return 1.0 if UNIFORM_WEIGHTING else num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Supply Server Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_optimizer_fn():\n",
    "    \n",
    "    return tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Default Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff.framework.set_default_executor(tff.framework.create_local_executor(max_fanout=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Iterative Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_process = (\n",
    "      tff.learning.federated_averaging.build_federated_averaging_process(\n",
    "          model_fn=model_fn,\n",
    "          server_optimizer_fn=server_optimizer_fn,\n",
    "          client_weight_fn=client_weight_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initalize the Process\n",
    "Server state will be updated in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Create Training Datsets from Randomly Sampled Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_clients(dataset, num_clients):\n",
    "    \n",
    "    random_indices = np.random.choice(len(dataset.client_ids), size=num_clients, replace=False)\n",
    "    \n",
    "    return np.array(dataset.client_ids)[random_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model Across Many Randomly Sampled Clients with Federated Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for round_num in tqdm(range(0, NUM_ROUNDS)):\n",
    "\n",
    "        # Examine validation metrics\n",
    "        print(f'Evaluating before training round #{round_num} on {NUM_VALIDATION_EXAMPLES} examples.')\n",
    "        keras_evaluate(server_state, val_data)\n",
    "\n",
    "        # Sample train clients to create a train dataset\n",
    "        print(f'Sampling {NUM_TRAIN_CLIENTS} new clients.')\n",
    "        train_clients = get_sample_clients(train_data, num_clients=NUM_TRAIN_CLIENTS)\n",
    "        train_datasets = [train_data.create_tf_dataset_for_client(client) for client in train_clients]\n",
    "\n",
    "        # Apply federated training round\n",
    "        server_state, server_metrics = iterative_process.next(server_state, train_datasets)\n",
    "\n",
    "        # Examine training metrics\n",
    "        print('Round: {}'.format(round_num))\n",
    "        print('   Loss: {:.8f}'.format(server_metrics.loss))\n",
    "        print('   num_batches: {}'.format(server_metrics.num_batches))\n",
    "        print('   num_examples: {}'.format(server_metrics.num_examples))\n",
    "        print('   num_tokens: {}'.format(server_metrics.num_tokens))\n",
    "        print('   num_tokens_no_oov: {}'.format(server_metrics.num_tokens_no_oov))\n",
    "        print('   accuracy: {:.5f}'.format(server_metrics.accuracy))\n",
    "        print('   accuracy_no_oov: {:.5f}'.format(server_metrics.accuracy_no_oov))\n",
    "\n",
    "        # Add train metrics to tracker\n",
    "        train_metrics_tracker.add_metrics_by_name('loss', server_metrics.loss)\n",
    "        train_metrics_tracker.add_metrics_by_name('accuracy', server_metrics.accuracy)\n",
    "        \n",
    "        # Save loss and accuracy from train and validation sets\n",
    "        np.save('train_loss.npy', train_metrics_tracker.get_metrics_by_name('loss'))\n",
    "        np.save('val_loss.npy', val_metrics_tracker.get_metrics_by_name('loss'))\n",
    "        np.save('train_accuracy.npy', train_metrics_tracker.get_metrics_by_name('accuracy'))\n",
    "        np.save('val_accuracy.npy', val_metrics_tracker.get_metrics_by_name('accuracy'))\n",
    "\n",
    "except KeyboardInterrupt as ke:\n",
    "    \n",
    "    print('Interrupted')\n",
    "    \n",
    "except:\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 15))\n",
    "    x_axis = range(0, NUM_ROUNDS)\n",
    "    ax.plot(x_axis, train_metrics_tracker.get_metrics_by_name('loss'), label='Train')\n",
    "    ax.plot(x_axis, val_metrics_tracker.get_metrics_by_name('loss'), label='Val')\n",
    "    ax.legend(loc='best', prop={'size': 15})\n",
    "    fig.suptitle('Model\\'s Loss with number of epochs', fontsize=20)\n",
    "    plt.xlabel('Epochs', fontsize=18)\n",
    "    plt.ylabel('Loss', fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 15))\n",
    "    ax2.plot(x_axis, train_metrics_tracker.get_metrics_by_name('accuracy'), label='Train')\n",
    "    ax2.plot(x_axis, val_metrics_tracker.get_metrics_by_name('accuracy'), label='Val')\n",
    "    ax2.legend(loc='best', prop={'size': 15})\n",
    "    fig2.suptitle('Model\\'s Accuracy with number of epochs', fontsize=20)\n",
    "    plt.xlabel('Epochs', fontsize=18)\n",
    "    plt.ylabel('Accuracy', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Loss and Accuracy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss_no_pt_embed = np.load('train_loss_no_pt_embed.npy')\n",
    "# train_accuracy_no_pt_embed = np.load('train_accuracy_no_pt_embed.npy')\n",
    "# val_loss_no_pt_embed = np.load('val_loss_no_pt_embed.npy')\n",
    "# val_accuracy_no_pt_embed = np.load('val_accuracy_no_pt_embed.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(20, 15))\n",
    "# x_axis = range(0, NUM_ROUNDS)\n",
    "# ax.plot(x_axis, train_loss_no_pt_embed, label='Train')\n",
    "# ax.plot(x_axis, val_loss_no_pt_embed, label='Val')\n",
    "# ax.plot(x_axis, train_metrics_tracker_with_embedding.get_metrics_by_name('loss'), label='Train with Glove')\n",
    "# ax.plot(x_axis, val_metrics_tracker_with_embedding.get_metrics_by_name('loss'), label='Val with Glove')\n",
    "# ax.legend(loc='best', prop={'size': 15})\n",
    "# fig.suptitle('Model\\'s Loss with number of epochs', fontsize=20)\n",
    "# plt.xlabel('Epochs', fontsize=18)\n",
    "# plt.ylabel('Loss', fontsize=18)\n",
    "# fig.savefig('Loss_with_Epochs.jpg')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(20, 15))\n",
    "# x_axis = range(0, NUM_ROUNDS)\n",
    "# ax.plot(x_axis, train_accuracy_no_pt_embed, label='Train')\n",
    "# ax.plot(x_axis, val_accuracy_no_pt_embed, label='Val')\n",
    "# ax.plot(x_axis, train_metrics_tracker_with_embedding.get_metrics_by_name('accuracy'), label='Train with Glove')\n",
    "# ax.plot(x_axis, val_metrics_tracker_with_embedding.get_metrics_by_name('accuracy'), label='Val with Glove')\n",
    "# ax.legend(loc='best', prop={'size': 15})\n",
    "# fig.suptitle('Model\\'s Accuracy with number of epochs', fontsize=20)\n",
    "# plt.xlabel('Epochs', fontsize=18)\n",
    "# plt.ylabel('Accuracy', fontsize=18)\n",
    "# fig.savefig('Accuracy_with_Epochs.jpg')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed",
   "language": "python",
   "name": "fed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
