{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Level Federated Text Generation with Stack Overflow with and without Pretrained Word Embeddings\n",
    "- 02-07-20\n",
    "\n",
    "**About:**\n",
    "\n",
    "This notebook loads the Stack Overflow data available through `tff.simulation.datasets` and trains an LSTM model with Federared Averaging by following the Federated Learning for Text Generation [example notebook](https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb).  The embedding layer in the LSTM is optionally initialized with either GloVe or FastText Embeddings.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- This notebook prepares the Stack Overflow dataset for word level language modeling using this [module](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/dataset.py\n",
    ").\n",
    "- The metrics for model training come from this [module](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/metrics.py). \n",
    "\n",
    "\n",
    "**Data:** \n",
    "- https://www.kaggle.com/stackoverflow/stackoverflow\n",
    "\n",
    "**License:** \n",
    "- https://creativecommons.org/licenses/by-sa/3.0/\n",
    "\n",
    "**Data and Model References:**\n",
    "- https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data\n",
    "- https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb\n",
    "- https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/baselines/stackoverflow\n",
    "- https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "**Environment Setup References:**\n",
    "- https://www.tensorflow.org/install/gpu\n",
    "- https://gist.github.com/matheustguimaraes/43e0b65aa534db4df2918f835b9b361d\n",
    "- https://www.tensorflow.org/install/source#tested_build_configurations\n",
    "- https://anbasile.github.io/programming/2017/06/25/jupyter-venv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Pip install these packages in the order listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade tensorflow-federated\n",
    "# !pip tensorflow\n",
    "# !pip install --upgrade tensorflow-gpu==2.0\n",
    "# !pip install --upgrade nltk\n",
    "# !pip install matplotlib\n",
    "# !pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, io\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import six\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.dataset as dataset\n",
    "import src.metrics as metrics\n",
    "import src.embeddings as embeddings\n",
    "import src.model as model\n",
    "import src.validation as validation\n",
    "import src.federated as federated\n",
    "import src.generate_text as generate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Compatability Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Tensorflow Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Built with Cuda: {}'.format(tf.test.is_built_with_cuda()))\n",
    "print('Build with GPU support: {}'.format(tf.test.is_built_with_gpu_support()))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Tensorflow to Use GPU, unless GPU isn't available (which is the case for me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices(device_type=None)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[-1], enable=True)\n",
    "for device in physical_devices:\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Some Parameters for Preprocessing the Data and Training the Model\n",
    "**Note:** Ask Keith how he's been setting there for internal experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "BATCH_SIZE = 16\n",
    "CLIENTS_EPOCHS_PER_ROUND = 1\n",
    "MAX_SEQ_LENGTH = 20\n",
    "MAX_ELEMENTS_PER_USER = 5000\n",
    "CENTRALIZED_TRAIN = False\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "NUM_VALIDATION_EXAMPLES = 10000\n",
    "NUM_TEST_EXAMPLES = 2\n",
    "\n",
    "NUM_ROUNDS = 5\n",
    "NUM_TRAIN_CLIENTS = 10\n",
    "UNIFORM_WEIGHTING = False\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "EMBEDDING_PATH = ''\n",
    "EMBEDDING_PATH = f'../word_embeddings/glove/glove.6B.{EMBEDDING_DIM}d.txt'\n",
    "#EMBEDDING_PATH = f'../word_embeddings/fasttext/wiki-news-{EMBEDDING_DIM}d-1M.vec'\n",
    "#EMBEDDING_PATH = f'../word_embeddings/fasttext/crawl-{EMBEDDING_DIM}d-2M.vec'\n",
    "\n",
    "\n",
    "RNN_UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Word Level Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel_stremmel/anaconda3/envs/tff/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = dataset.construct_word_level_datasets(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    client_epochs_per_round=CLIENTS_EPOCHS_PER_ROUND,\n",
    "    max_seq_len=MAX_SEQ_LENGTH,\n",
    "    max_elements_per_user=MAX_ELEMENTS_PER_USER,\n",
    "    centralized_train=CENTRALIZED_TRAIN,\n",
    "    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
    "    num_validation_examples=NUM_VALIDATION_EXAMPLES,\n",
    "    num_test_examples=NUM_TEST_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Dataset Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dataset.get_vocab(vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Special Characters Created During Preprocessing\n",
    "The four special tokens are:\n",
    "- pad: padding token\n",
    "- oov: out of vocabulary\n",
    "- bos: begin of sentence\n",
    "- eos: end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad, oov, bos, eos = dataset.get_special_tokens(vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "special2idx = dict(zip(['pad', 'oov', 'bos', 'eos'], [pad, oov, bos, eos]))\n",
    "idx2special = {v:k for k, v in special2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Vocabulary\n",
    "Add one to account for the pad token which has idx 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word:i+1 for i, word in enumerate(vocab)}\n",
    "idx2word = {i+1:word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {**word2idx, **special2idx}\n",
    "idx2word = {**idx2word, **idx2special}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Vocab Size\n",
    "This accounts for having added the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENDED_VOCAB_SIZE = VOCAB_SIZE + len(special2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Word Embeddings\n",
    "- Either [GloVe embeddings from Stanford](https://nlp.stanford.edu/projects/glove/) - license [here](https://www.opendatacommons.org/licenses/pddl/1.0/)\n",
    "- Or [FastText embeddings from Facebook](https://fasttext.cc/docs/en/english-vectors.html) - license [here](https://creativecommons.org/licenses/by-sa/3.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embedding Matrix from Words in Word Index and Word Embeddings\n",
    "If `EMBEDDING_PATH` is an empy string, the embedding matrix in the embedding layer is initialized according to the random uniform distribution used by the tf.keras embedding layer by passing the 'uniform' string as an argument to the `embedding_initializer` in the `build_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10003"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'the'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d2e2712a7007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0membedding_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDING_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         word2idx=word2idx)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'uniform'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev_gear/git/fl-text-models/src/embeddings.py\u001b[0m in \u001b[0;36mcreate_matrix_from_pretrained_embeddings\u001b[0;34m(embedding_fn, embedding_path, embedding_dim, word2idx)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0membedding_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'here'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev_gear/git/fl-text-models/src/embeddings.py\u001b[0m in \u001b[0;36mload_fasttext_embeddings\u001b[0;34m(embedding_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0membedding_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev_gear/git/fl-text-models/src/embeddings.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0membedding_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'the'"
     ]
    }
   ],
   "source": [
    "if EMBEDDING_PATH:\n",
    "    embedding_matrix=embeddings.create_matrix_from_pretrained_embeddings(\n",
    "        embedding_fn=embeddings.load_fasttext_embeddings,\n",
    "        embedding_path=EMBEDDING_PATH,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        word2idx=word2idx)\n",
    "else:\n",
    "    embedding_matrix = 'uniform'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or Build the Model and Try Generating Some Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = model.build_model(extended_vocab_size=EXTENDED_VOCAB_SIZE,\n",
    "                                embedding_dim=EMBEDDING_DIM,\n",
    "                                embedding_matrix=embedding_matrix,\n",
    "                                rnn_units=RNN_UNITS)\n",
    "\n",
    "generate_text.generate_text(model=keras_model,\n",
    "                            start_string='How are you today'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFF Version of the Model to be Trained with Federated Averaging\n",
    "- TFF uses a sample batch so it knows the types and shapes that your model expects.\n",
    "- The model function then builds and compiles the model and creates a TFF version to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(val_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Train and Validation Model Trackers to be Used Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metric_names = ['loss',\n",
    "                           'num_tokens',\n",
    "                           'num_tokens_no_oov',\n",
    "                           'num_batches',\n",
    "                           'num_examples',\n",
    "                           'accuracy',\n",
    "                           'accuracy_no_oov',\n",
    "                           'accuracy_no_oov_no_oes']\n",
    "\n",
    "train_metrics_tracker = validation.model_history_tracker(metric_names=evaluation_metric_names)\n",
    "val_metrics_tracker = validation.model_history_tracker(metric_names=evaluation_metric_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Default Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff.framework.set_default_executor(tff.framework.create_local_executor(max_fanout=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Iterative Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_process = (\n",
    "      tff.learning.federated_averaging.build_federated_averaging_process(\n",
    "          model_fn=model_fn(extended_vocab_size=EXTENDED_VOCAB_SIZE,\n",
    "                            embedding_sim=EMBEDDING_DIM,\n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            rnn_units=RNN_UNITS,\n",
    "                            vocab_size=vocab_size,\n",
    "                            sample_batch=sample_batch),\n",
    "          server_optimizer_fn=federated.server_optimizer_fn,\n",
    "          client_weight_fn=federated.client_weight_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initalize the Process\n",
    "Server state will be updated in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model Across Many Randomly Sampled Clients with Federated Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for round_num in range(0, NUM_ROUNDS):\n",
    "\n",
    "        # Examine validation metrics\n",
    "        print(f'Evaluating before training round #{round_num} on {NUM_VALIDATION_EXAMPLES} examples.')\n",
    "        validation.keras_evaluate(state=server_state,\n",
    "                                  val_dataset=val_dataset,\n",
    "                                  extended_vocab_size=EXTENDED_VOCAB_SIZE,\n",
    "                                  vocab_size=VOCAB_SIZE,\n",
    "                                  embedding_dim=EMBEDDING_DIM,\n",
    "                                  embedding_matrix=embedding_matrix,\n",
    "                                  rnn_units=RNN_UNITS,\n",
    "                                  metrics_tracker=val_metrics_tracker\n",
    "\n",
    "        # Sample train clients to create a train dataset\n",
    "        print(f'Sampling {NUM_TRAIN_CLIENTS} new clients.')\n",
    "        train_clients = federated.get_sample_clients(dataset=train_data, num_clients=NUM_TRAIN_CLIENTS)\n",
    "        train_datasets = [train_data.create_tf_dataset_for_client(client) for client in train_clients]\n",
    "\n",
    "        # Apply federated training round\n",
    "        server_state, server_metrics = iterative_process.next(server_state, train_datasets)\n",
    "\n",
    "        # Examine training metrics\n",
    "        print('Round: {}'.format(round_num))\n",
    "        print('   Loss: {:.8f}'.format(server_metrics.loss))\n",
    "        print('   num_batches: {}'.format(server_metrics.num_batches))\n",
    "        print('   num_examples: {}'.format(server_metrics.num_examples))\n",
    "        print('   num_tokens: {}'.format(server_metrics.num_tokens))\n",
    "        print('   num_tokens_no_oov: {}'.format(server_metrics.num_tokens_no_oov))\n",
    "        print('   accuracy: {:.5f}'.format(server_metrics.accuracy))\n",
    "        print('   accuracy_no_oov: {:.5f}'.format(server_metrics.accuracy_no_oov))\n",
    "\n",
    "        # Add train metrics to tracker\n",
    "        train_metrics_tracker.add_metrics_by_name('loss', server_metrics.loss)\n",
    "        train_metrics_tracker.add_metrics_by_name('accuracy', server_metrics.accuracy)\n",
    "        \n",
    "        # Save loss and accuracy from train and validation sets\n",
    "        np.save('train_loss.npy', train_metrics_tracker.get_metrics_by_name('loss'))\n",
    "        np.save('val_loss.npy', val_metrics_tracker.get_metrics_by_name('loss'))\n",
    "        np.save('train_accuracy.npy', train_metrics_tracker.get_metrics_by_name('accuracy'))\n",
    "        np.save('val_accuracy.npy', val_metrics_tracker.get_metrics_by_name('accuracy'))\n",
    "\n",
    "except KeyboardInterrupt as ke:\n",
    "    \n",
    "    print('Interrupted')\n",
    "    \n",
    "except:\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 15))\n",
    "    x_axis = range(0, NUM_ROUNDS)\n",
    "    ax.plot(x_axis, train_metrics_tracker.get_metrics_by_name('loss'), label='Train')\n",
    "    ax.plot(x_axis, val_metrics_tracker.get_metrics_by_name('loss'), label='Val')\n",
    "    ax.legend(loc='best', prop={'size': 15})\n",
    "    fig.suptitle('Loss by Epoch', fontsize=20)\n",
    "    plt.xlabel('Epochs', fontsize=18)\n",
    "    plt.ylabel('Loss', fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 15))\n",
    "    ax2.plot(x_axis, train_metrics_tracker.get_metrics_by_name('accuracy'), label='Train')\n",
    "    ax2.plot(x_axis, val_metrics_tracker.get_metrics_by_name('accuracy'), label='Val')\n",
    "    ax2.legend(loc='best', prop={'size': 15})\n",
    "    fig2.suptitle('Accuracy by Epoch', fontsize=20)\n",
    "    plt.xlabel('Epochs', fontsize=18)\n",
    "    plt.ylabel('Accuracy', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Loss and Accuracy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss_no_pt_embed = np.load('train_loss_no_pt_embed.npy')\n",
    "# train_accuracy_no_pt_embed = np.load('train_accuracy_no_pt_embed.npy')\n",
    "# val_loss_no_pt_embed = np.load('val_loss_no_pt_embed.npy')\n",
    "# val_accuracy_no_pt_embed = np.load('val_accuracy_no_pt_embed.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(20, 15))\n",
    "# x_axis = range(0, NUM_ROUNDS)\n",
    "# ax.plot(x_axis, train_loss_no_pt_embed, label='Train')\n",
    "# ax.plot(x_axis, val_loss_no_pt_embed, label='Val')\n",
    "# ax.plot(x_axis, train_metrics_tracker_with_embedding.get_metrics_by_name('loss'), label='Train with Glove')\n",
    "# ax.plot(x_axis, val_metrics_tracker_with_embedding.get_metrics_by_name('loss'), label='Val with Glove')\n",
    "# ax.legend(loc='best', prop={'size': 15})\n",
    "# fig.suptitle('Model\\'s Loss with number of epochs', fontsize=20)\n",
    "# plt.xlabel('Epochs', fontsize=18)\n",
    "# plt.ylabel('Loss', fontsize=18)\n",
    "# fig.savefig('Loss_with_Epochs.jpg')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(20, 15))\n",
    "# x_axis = range(0, NUM_ROUNDS)\n",
    "# ax.plot(x_axis, train_accuracy_no_pt_embed, label='Train')\n",
    "# ax.plot(x_axis, val_accuracy_no_pt_embed, label='Val')\n",
    "# ax.plot(x_axis, train_metrics_tracker_with_embedding.get_metrics_by_name('accuracy'), label='Train with Glove')\n",
    "# ax.plot(x_axis, val_metrics_tracker_with_embedding.get_metrics_by_name('accuracy'), label='Val with Glove')\n",
    "# ax.legend(loc='best', prop={'size': 15})\n",
    "# fig.suptitle('Model\\'s Accuracy with number of epochs', fontsize=20)\n",
    "# plt.xlabel('Epochs', fontsize=18)\n",
    "# plt.ylabel('Accuracy', fontsize=18)\n",
    "# fig.savefig('Accuracy_with_Epochs.jpg')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed",
   "language": "python",
   "name": "fed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
