{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Level Federated Text Generation with Stack Overflow with Randomly Initialized or Pretrained Word Embeddings\n",
    "- 03-05-20\n",
    "- Runs on GCP and local Ubuntu 16.04\n",
    "\n",
    "**About:**\n",
    "\n",
    "This notebook loads the Stack Overflow data available through `tff.simulation.datasets` and trains an LSTM model with Federared Averaging by following the Federated Learning for Text Generation [example notebook](https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb).  The embedding layer is initialized with one of the following options by setting the `EMBEDDING_LAYER` parameter:\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/) ([license here](https://www.opendatacommons.org/licenses/pddl/1.0/))\n",
    "- [FastText](https://fasttext.cc/docs/en/english-vectors.html) ([license here](https://creativecommons.org/licenses/by-sa/3.0/))\n",
    "- [GPT-2](https://openai.com/blog/better-language-models/) ([license here](https://github.com/huggingface/transformers/blob/master/LICENSE))\n",
    "- [Randomly initialized embeddings](https://www.tensorflow.org/api_docs/python/tf/random_uniform_initializer)  \n",
    "\n",
    "After downloading the GloVe or FastText embeddings, place the embedding files at the top level of the repository in directories called `word_embedding/glove` and `word_embedding/fasttext` respectively.  GPT-2 embeddings are downloaded by running the notebook which makes a call to `src/embeddings.py` to download the embeddings from [huggingface](https://github.com/huggingface/transformers).  \n",
    "\n",
    "**Code from Tensorflow Federated:**\n",
    "\n",
    "- This notebook prepares the Stack Overflow dataset for word level language modeling using this [module](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/dataset.py\n",
    ").\n",
    "- The metrics for model training come from this [module](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/baselines/stackoverflow/metrics.py). \n",
    "\n",
    "\n",
    "**Data and Model References:**\n",
    "- [TFF Stack Overflow `load_data`](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data)\n",
    "- [TFF text generation tutorial](https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb)\n",
    "- [Google TFF team research baselines for Stack Overflow](https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/baselines/stackoverflow)\n",
    "- [Tensorflow text generation tutorial](https://www.tensorflow.org/tutorials/text/text_generation)\n",
    "\n",
    "**Environment Setup References:**\n",
    "- [Installing Tensorflow for GPU](https://www.tensorflow.org/install/gpu)\n",
    "- [Install CUDA 10.0 and cuDNN v7.4.2 on Ubuntu 16.04](https://gist.github.com/matheustguimaraes/43e0b65aa534db4df2918f835b9b361d)\n",
    "- [Tensorflow build configs](https://www.tensorflow.org/install/source#tested_build_configurations)\n",
    "- [Using jupyter notebooks with a virtual environment](https://anbasile.github.io/programming/2017/06/25/jupyter-venv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, io\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import six\n",
    "import time\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import dataset, metrics, embeddings, model, validation, federated, generate_text, transfer_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Some Parameters for Preprocessing the Data and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "BATCH_SIZE = 16\n",
    "CLIENTS_EPOCHS_PER_ROUND = 1\n",
    "MAX_SEQ_LENGTH = 20\n",
    "MAX_ELEMENTS_PER_USER = 5000\n",
    "CENTRALIZED_TRAIN = False\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "NUM_VALIDATION_EXAMPLES = 20000\n",
    "NUM_TEST_EXAMPLES = 100\n",
    "\n",
    "NUM_ROUNDS = 10\n",
    "NUM_TRAIN_CLIENTS = 10\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "RNN_UNITS = 128\n",
    "\n",
    "EMBEDDING_LAYER = 'pp_pca_pp_gpt2'\n",
    "SAV = 'embedding_layer_results/pretrainedNet_{}_{}_{}_{}/'.format(EMBEDDING_LAYER, \n",
    "                                                    EMBEDDING_DIM, \n",
    "                                                    RNN_UNITS, \n",
    "                                                    EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Output Directory if Nonexistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAV):\n",
    "    os.makedirs(SAV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Word Level Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel_stremmel/anaconda3/envs/tff/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:64: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = dataset.construct_word_level_datasets(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    client_epochs_per_round=CLIENTS_EPOCHS_PER_ROUND,\n",
    "    max_seq_len=MAX_SEQ_LENGTH,\n",
    "    max_elements_per_user=MAX_ELEMENTS_PER_USER,\n",
    "    centralized_train=CENTRALIZED_TRAIN,\n",
    "    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
    "    num_validation_examples=NUM_VALIDATION_EXAMPLES,\n",
    "    num_test_examples=NUM_TEST_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Dataset Vocab and Extended Vocab Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dataset.get_vocab(vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad, oov, bos, eos = dataset.get_special_token_words()\n",
    "extended_vocab = [pad] + vocab + [oov, bos, eos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_vocab_size = len(extended_vocab)\n",
    "word2idx = {word: i for i, word in enumerate(extended_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embedding Matrix from Words in Word Index and Word Embeddings\n",
    "If the `EMBEDDING_LAYER` option is set to 'random', the embedding matrix in the embedding layer is initialized according to the random uniform distribution used by the tf.keras embedding layer by passing the 'uniform' string as an argument to the `embedding_initializer` in the `build_model` function.  Otherwise, an embedding index called `word2embedding` is created from pretrained embeddings either loaded from the 'word_embeddings' directory or created from a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDING_LAYER == 'random':\n",
    "    pass\n",
    "\n",
    "elif EMBEDDING_LAYER == 'glove':\n",
    "    embedding_path = '../word_embeddings/glove/glove.6B.{}d.txt'.format(EMBEDDING_DIM)\n",
    "    word2embedding = embeddings.load_embeddings(embedding_path)\n",
    "    \n",
    "elif EMBEDDING_LAYER == 'fasttext':\n",
    "    embedding_path = '../word_embeddings/fasttext/wiki-news-300d-1M.vec'\n",
    "    word2embedding = embeddings.load_embeddings(embedding_path)\n",
    "    \n",
    "elif EMBEDDING_LAYER == 'pca_fasttext':\n",
    "    embedding_path = '../word_embeddings/fasttext/wiki-news-300d-1M.vec'\n",
    "    word2embedding = embeddings.load_embeddings(embedding_path)\n",
    "    word2embedding = embeddings.to_pca_projections(word2embedding, vocab, EMBEDDING_DIM)\n",
    "\n",
    "elif EMBEDDING_LAYER == 'pp_pca_pp_fasttext':\n",
    "    embedding_path = '../word_embeddings/fasttext/wiki-news-300d-1M.vec'\n",
    "    word2embedding = embeddings.load_embeddings(embedding_path)\n",
    "    word2embedding = embeddings.to_pp_pca_pp_projections(word2embedding, vocab, EMBEDDING_DIM)\n",
    "    \n",
    "elif EMBEDDING_LAYER == 'gpt2':\n",
    "    word2embedding = embeddings.create_gpt_embeddings(vocab)\n",
    "    word2embedding = embeddings.to_pca_projections(word2embedding, vocab, EMBEDDING_DIM)\n",
    "\n",
    "elif EMBEDDING_LAYER == 'pca_gpt2':\n",
    "    word2embedding = embeddings.create_gpt_embeddings(vocab)\n",
    "    word2embedding = embeddings.to_pca_projections(word2embedding, vocab, EMBEDDING_DIM)\n",
    "\n",
    "elif EMBEDDING_LAYER == 'pp_pca_pp_gpt2':\n",
    "    word2embedding = embeddings.create_gpt_embeddings(vocab)\n",
    "    word2embedding = embeddings.to_pp_pca_pp_projections(word2embedding, vocab, EMBEDDING_DIM)\n",
    "    \n",
    "else:\n",
    "    layer_opts = ['random', 'glove',\n",
    "                  'fasttext', 'pca_fasttext', 'pp_pca_pp_fasttext', \n",
    "                  'gpt2', 'pca_gpt2', 'pp_pca_pp_gpt2']\n",
    "    \n",
    "    raise ValueError(\"EMBEDDING LAYER must be in {}.\".format(layer_opts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel_stremmel/Documents/dev_gear/git/fl-text-models/src/embeddings.py:80: missing_words_warning: 5 words set to default random initialization\n",
      "  .format(missing), missing_words_warning)\n"
     ]
    }
   ],
   "source": [
    "if EMBEDDING_LAYER == 'random':\n",
    "    embedding_matrix = 'uniform'\n",
    "else:\n",
    "    embedding_matrix = embeddings.create_matrix_from_pretrained_embeddings(\n",
    "        word2embedding=word2embedding,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain with a Different Text Corpus by First Reading in the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../raw_shakespeare_data.txt') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Words in the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = []\n",
    "for word in data.split():\n",
    "    if not word2idx.get(word, None): # return None if not found\n",
    "        encoded.append(0) # 0 is the pad token\n",
    "    else:\n",
    "        encoded.append(word2idx[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "for i in range(len(encoded)-1):\n",
    "    X.append(encoded[i])\n",
    "    Y.append(encoded[i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(X, 1)\n",
    "Y = np.expand_dims(Y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_sp = model.build_model(extended_vocab_size=extended_vocab_size,\n",
    "                                   embedding_dim=EMBEDDING_DIM,\n",
    "                                   embedding_matrix=embedding_matrix,\n",
    "                                   rnn_units=RNN_UNITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7fe4942c5650>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics_sp = validation.get_metrics(VOCAB_SIZE)\n",
    "model.compile_model(keras_model_sp, evaluation_metrics_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model1_input (InputLayer)    [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "model1_embedding (Embedding) (None, None, 50)          500200    \n",
      "_________________________________________________________________\n",
      "model1_lstm (LSTM)           (None, None, 128)         91648     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 50)          6450      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, None, 10004)       510204    \n",
      "=================================================================\n",
      "Total params: 1,108,502\n",
      "Trainable params: 1,108,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model_sp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 904064 samples\n",
      "Epoch 1/5\n",
      "867488/904064 [===========================>..] - ETA: 8s - loss: 1.6733 - num_tokens: 232152.0000 - num_tokens_no_oov: 232152.0000 - num_batches: 27109.0000 - num_examples: 867488.0000 - accuracy: 0.0090 - accuracy_no_oov: 0.0090 - accuracy_no_oov_no_eos: 0.0090"
     ]
    }
   ],
   "source": [
    "history = keras_model_sp.fit(X, Y, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFF Version of the Model to be Trained with Federated Averaging\n",
    "- TFF uses a sample batch so it knows the types and shapes that your model expects.\n",
    "- The model function then builds and compiles the model and creates a TFF version to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(val_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Train and Validation Model Trackers to be Used Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metric_names = ['loss',\n",
    "                           'num_tokens',\n",
    "                           'num_tokens_no_oov',\n",
    "                           'num_batches',\n",
    "                           'num_examples',\n",
    "                           'accuracy',\n",
    "                           'accuracy_no_oov',\n",
    "                           'accuracy_no_oov_no_oes']\n",
    "\n",
    "train_metrics_tracker = validation.model_history_tracker(metric_names=evaluation_metric_names)\n",
    "val_metrics_tracker = validation.model_history_tracker(metric_names=evaluation_metric_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Iterative Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_process = (\n",
    "      tff.learning.federated_averaging.build_federated_averaging_process(\n",
    "          model_fn=lambda : model.model_fn(extended_vocab_size=extended_vocab_size,\n",
    "                                           embedding_dim=EMBEDDING_DIM,\n",
    "                                           embedding_matrix=embedding_matrix,\n",
    "                                           rnn_units=RNN_UNITS,\n",
    "                                           vocab_size=VOCAB_SIZE,\n",
    "                                           sample_batch=sample_batch),\n",
    "          server_optimizer_fn=federated.server_optimizer_fn,\n",
    "          client_weight_fn=federated.client_weight_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following is the step where the transfer learning actually takes place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_state = transfer_learning.learn_from_pretrained_model(iterative_process, keras_model_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model Across Many Randomly Sampled Clients with Federated Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for round_num in tqdm(range(0, NUM_ROUNDS)):\n",
    "\n",
    "    # Examine validation metrics\n",
    "    print('Evaluating before round #{} on {} examples.'.format(round_num, NUM_VALIDATION_EXAMPLES))\n",
    "    validation.keras_evaluate(state=server_state,\n",
    "                              val_dataset=val_data,\n",
    "                              extended_vocab_size=extended_vocab_size,\n",
    "                              vocab_size=VOCAB_SIZE,\n",
    "                              embedding_dim=EMBEDDING_DIM,\n",
    "                              embedding_matrix=embedding_matrix,\n",
    "                              rnn_units=RNN_UNITS,\n",
    "                              metrics_tracker=val_metrics_tracker,\n",
    "                              checkpoint_dir=SAV)\n",
    "\n",
    "    # Sample train clients to create a train dataset\n",
    "    print('\\nSampling {} new clients.'.format(NUM_TRAIN_CLIENTS))\n",
    "    train_clients = federated.get_sample_clients(dataset=train_data, num_clients=NUM_TRAIN_CLIENTS)\n",
    "    train_datasets = [train_data.create_tf_dataset_for_client(client) for client in train_clients]\n",
    "\n",
    "    # Apply federated training round\n",
    "    server_state, server_metrics = iterative_process.next(server_state, train_datasets)\n",
    "\n",
    "    # Examine training metrics\n",
    "    print('Round: {}'.format(round_num))\n",
    "    print('   Loss: {:.8f}'.format(server_metrics.loss))\n",
    "    print('   num_batches: {}'.format(server_metrics.num_batches))\n",
    "    print('   num_examples: {}'.format(server_metrics.num_examples))\n",
    "    print('   num_tokens: {}'.format(server_metrics.num_tokens))\n",
    "    print('   num_tokens_no_oov: {}'.format(server_metrics.num_tokens_no_oov))\n",
    "    print('   accuracy: {:.5f}'.format(server_metrics.accuracy))\n",
    "    print('   accuracy_no_oov: {:.5f}'.format(server_metrics.accuracy_no_oov))\n",
    "\n",
    "    # Add train metrics to tracker\n",
    "    train_metrics_tracker.add_metrics_by_name('loss', server_metrics.loss)\n",
    "    train_metrics_tracker.add_metrics_by_name('accuracy', server_metrics.accuracy)\n",
    "    train_metrics_tracker.add_metrics_by_name('num_examples', server_metrics.num_examples)\n",
    "    train_metrics_tracker.add_metrics_by_name('num_tokens', server_metrics.num_tokens)\n",
    "    train_metrics_tracker.add_metrics_by_name('num_tokens_no_oov', server_metrics.num_tokens_no_oov)\n",
    "        \n",
    "    # Save loss and accuracy from train and validation sets\n",
    "    np.save(SAV + 'train_loss.npy', train_metrics_tracker.get_metrics_by_name('loss'))\n",
    "    np.save(SAV + 'val_loss.npy', val_metrics_tracker.get_metrics_by_name('loss'))\n",
    "    np.save(SAV + 'train_accuracy.npy', train_metrics_tracker.get_metrics_by_name('accuracy'))\n",
    "    np.save(SAV + 'val_accuracy.npy', val_metrics_tracker.get_metrics_by_name('accuracy'))\n",
    "        \n",
    "    # Save train sample stats\n",
    "    np.save(SAV + 'num_examples.npy', train_metrics_tracker.get_metrics_by_name('num_examples'))\n",
    "    np.save(SAV + 'num_tokens.npy', train_metrics_tracker.get_metrics_by_name('num_tokens'))\n",
    "    np.save(SAV + 'num_tokens_no_oov.npy', train_metrics_tracker.get_metrics_by_name('num_tokens_no_oov'))\n",
    "    \n",
    "    # Write time since start of training\n",
    "    with open(SAV + 'train_time.txt', 'a+') as f: \n",
    "        f.write('{}\\n'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Plot Title Based on Training Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_config = 'Clients: {}, Max Elements per Client: {}, Max Seq Len: {}, Num Rounds: {}'.format(\n",
    "    NUM_TRAIN_CLIENTS, MAX_ELEMENTS_PER_USER, MAX_SEQ_LENGTH, NUM_ROUNDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Train and Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "x_axis = range(0, NUM_ROUNDS)\n",
    "ax.plot(x_axis, train_metrics_tracker.get_metrics_by_name('loss'), label='Train')\n",
    "ax.plot(x_axis, val_metrics_tracker.get_metrics_by_name('loss'), label='Val')\n",
    "ax.legend(loc='best', prop={'size': 15})\n",
    "plt.title('Loss by Epoch - {}'.format(round_config), fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAV + 'Loss by Epoch.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Train and Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "x_axis = range(0, NUM_ROUNDS)\n",
    "ax.plot(x_axis, train_metrics_tracker.get_metrics_by_name('accuracy'), label='Train')\n",
    "ax.plot(x_axis, val_metrics_tracker.get_metrics_by_name('accuracy'), label='Val')\n",
    "ax.legend(loc='best', prop={'size': 15})\n",
    "plt.title('Accuracy by Epoch - {}'.format(round_config), fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Accuracy', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAV + 'Accuracy by Epoch.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed",
   "language": "python",
   "name": "fed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
